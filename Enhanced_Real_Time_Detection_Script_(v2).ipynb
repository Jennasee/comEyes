{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mss\n",
        "import psutil\n",
        "import pyautogui # Keep for potential future use, but not used in current drawing logic\n",
        "from PySide6 import QtCore, QtGui, QtWidgets\n",
        "from PySide6.QtCore import Qt, QTimer, QPoint, QPointF, QThread, Signal, Slot, QMutex, QMutexLocker, QObject, QRect\n",
        "from PySide6.QtGui import QColor, QFont, QPainter, QPen, QFontDatabase, QImage, QPixmap, QPolygonF\n",
        "from PySide6.QtWidgets import (QApplication, QWidget, QMainWindow, QVBoxLayout, QGridLayout,\n",
        "                             QComboBox, QSpinBox, QDoubleSpinBox, QPushButton, QLabel,\n",
        "                             QTextEdit, QCheckBox, QGroupBox)\n",
        "import logging\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import os\n",
        "import collections # Added for deque\n",
        "\n",
        "# --- Configurations ---\n",
        "UPDATE_INTERVAL_MS = 500  # Interval for updating HUD text elements\n",
        "SCANLINE_SPEED_MS = 15    # Speed of the scanline effect\n",
        "# Adjusted capture interval - 1ms is often too fast for processing to keep up\n",
        "# Set to 10ms (100 FPS target capture rate), processing threads will run as fast as possible\n",
        "CAPTURE_INTERVAL_MS = 10\n",
        "SYSTEM_INFO_INTERVAL_MS = 1000 # Interval for updating system info\n",
        "DEFAULT_CONFIDENCE_THRESHOLD = 0.4 # Default confidence threshold for detection\n",
        "DEFAULT_NMS_THRESHOLD = 0.3        # Default NMS threshold\n",
        "DEFAULT_POSE_CONFIDENCE_THRESHOLD = 0.5 # Default confidence for pose keypoints\n",
        "FONT_NAME = \"Press Start 2P\"\n",
        "FALLBACK_FONT = \"Monospace\"\n",
        "FONT_SIZE_SMALL = 10\n",
        "FONT_SIZE_MEDIUM = 12\n",
        "RED_COLOR = QColor(255, 0, 0)\n",
        "GREEN_COLOR = QColor(0, 255, 0)\n",
        "BLUE_COLOR = QColor(0, 0, 255)\n",
        "YELLOW_COLOR = QColor(255, 255, 0)\n",
        "CYAN_COLOR = QColor(0, 255, 255)\n",
        "MAGENTA_COLOR = QColor(255, 0, 255)\n",
        "TEXT_COLOR = QColor(255, 0, 0)\n",
        "\n",
        "# --- Smoothing Parameters ---\n",
        "TARGET_CENTER_SMOOTHING_FACTOR = 0.3 # Alpha for EMA (lower = smoother, more lag)\n",
        "TRAJECTORY_SMOOTHING_FACTOR = 0.4    # Alpha for EMA for trajectory prediction\n",
        "PATH_HISTORY_LENGTH = 30             # Number of past points to store for path tracking (reduced for performance)\n",
        "TRAJECTORY_PREDICTION_POINTS = 5     # Number of recent points to use for velocity calculation\n",
        "TRAJECTORY_PREDICTION_DURATION = 0.3 # Seconds into the future to predict (reduced for performance)\n",
        "\n",
        "# --- Model Names ---\n",
        "OBJECT_DETECTION_MODEL = 'yolov8n.pt'\n",
        "POSE_ESTIMATION_MODEL = 'yolov8n-pose.pt'\n",
        "\n",
        "# --- Pose Estimation Keypoint Connections (COCO format) ---\n",
        "# Define connections between keypoints to draw skeletons\n",
        "# Based on COCO keypoint definition used by YOLOv8 pose models\n",
        "# (Check model documentation for specific keypoint indices if needed)\n",
        "POSE_CONNECTIONS = [\n",
        "    (0, 1), (0, 2), (1, 3), (2, 4),  # Head\n",
        "    (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # Torso/Arms\n",
        "    (11, 12), (5, 11), (6, 12), # Shoulders to Hips\n",
        "    (11, 13), (13, 15), (12, 14), (14, 16)  # Legs\n",
        "]\n",
        "# Colors for different keypoints/limbs (example)\n",
        "POSE_COLORS = [QColor(255, 0, 0), QColor(0, 255, 0), QColor(0, 0, 255),\n",
        "               QColor(255, 255, 0), QColor(0, 255, 255), QColor(255, 0, 255)]\n",
        "\n",
        "\n",
        "# --- Logging Setup ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(threadName)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def random_hex(length):\n",
        "    \"\"\"Generate a random hexadecimal string of specified length.\"\"\"\n",
        "    return ''.join(random.choice('ABCDEF0123456789') for _ in range(length))\n",
        "\n",
        "def get_gpu_info():\n",
        "    \"\"\"Fetches GPU name and memory usage if CUDA is available.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        try:\n",
        "            total_mem, free_mem = torch.cuda.mem_get_info(0)\n",
        "            used_mem = total_mem - free_mem\n",
        "            mem_usage = f\"{(used_mem / (1024**3)):.1f}/{(total_mem / (1024**3)):.1f} GB\"\n",
        "        except AttributeError:\n",
        "            total_mem = torch.cuda.get_device_properties(0).total_memory\n",
        "            used_mem = torch.cuda.memory_allocated(0)\n",
        "            mem_usage = f\"{(used_mem / (1024**3)):.1f}/{(total_mem / (1024**3)):.1f} GB (Allocated)\"\n",
        "        except Exception as e:\n",
        "             logger.error(f\"Error getting GPU memory info: {e}\", exc_info=True)\n",
        "             mem_usage = \"N/A (Error reading memory)\"\n",
        "        return gpu_name, mem_usage\n",
        "    return \"N/A (CUDA not available)\", \"N/A\"\n",
        "\n",
        "# --- Custom Logging Handler ---\n",
        "class QTextEditLogger(logging.Handler):\n",
        "    \"\"\"Sends log records to a QTextEdit widget in a thread-safe manner.\"\"\"\n",
        "    def __init__(self, text_edit):\n",
        "        super().__init__()\n",
        "        self.text_edit = text_edit\n",
        "        self.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
        "\n",
        "    def emit(self, record):\n",
        "        msg = self.format(record)\n",
        "        QtCore.QMetaObject.invokeMethod(\n",
        "            self.text_edit,\n",
        "            \"append\",\n",
        "            QtCore.Qt.QueuedConnection,\n",
        "            QtCore.Q_ARG(str, msg)\n",
        "        )\n",
        "\n",
        "# --- Screen Capture Thread ---\n",
        "class ScreenCaptureThread(QThread):\n",
        "    \"\"\"Captures the screen at a specified interval.\"\"\"\n",
        "    frame_ready = Signal(np.ndarray, float) # Emit frame (BGR) and capture timestamp\n",
        "    status_update = Signal(str)\n",
        "\n",
        "    def __init__(self, monitor_spec):\n",
        "        super().__init__()\n",
        "        self.monitor_spec = monitor_spec\n",
        "        self.running = False\n",
        "        self.sct = None\n",
        "        self._capture_interval_ms = CAPTURE_INTERVAL_MS\n",
        "        self._lock = QMutex()\n",
        "\n",
        "    def run(self):\n",
        "        self.running = True\n",
        "        try:\n",
        "            self.sct = mss.mss()\n",
        "            logger.info(f\"Screen capture started for monitor: {self.monitor_spec}\")\n",
        "            last_capture_time = time.perf_counter() # Use high-resolution timer\n",
        "\n",
        "            while self.running:\n",
        "                capture_start_time = time.perf_counter()\n",
        "                try:\n",
        "                    sct_img = self.sct.grab(self.monitor_spec)\n",
        "                    frame = np.array(sct_img)\n",
        "\n",
        "                    if frame.shape[2] == 4:\n",
        "                        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_BGRA2BGR)\n",
        "                    elif frame.shape[2] == 3:\n",
        "                        frame_bgr = frame\n",
        "                    else:\n",
        "                        logger.warning(f\"Unexpected frame channel count: {frame.shape[2]}\")\n",
        "                        continue\n",
        "\n",
        "                    current_time = time.time() # Use system time for timestamping frame data\n",
        "                    self.frame_ready.emit(frame_bgr, current_time)\n",
        "\n",
        "                except mss.ScreenShotError as e:\n",
        "                    self.status_update.emit(f\"Screen capture error: {e}\")\n",
        "                    logger.error(f\"Screen capture error: {e}\")\n",
        "                    time.sleep(1) # Wait before retrying on error\n",
        "                except Exception as e:\n",
        "                    self.status_update.emit(f\"Unexpected screen capture error: {e}\")\n",
        "                    logger.error(f\"Unexpected screen capture error: {e}\", exc_info=True)\n",
        "                    time.sleep(1)\n",
        "\n",
        "                # Calculate sleep time based on high-resolution timer\n",
        "                elapsed = time.perf_counter() - capture_start_time\n",
        "                with QMutexLocker(self._lock):\n",
        "                    interval_sec = self._capture_interval_ms / 1000.0\n",
        "                sleep_time = max(0, interval_sec - elapsed)\n",
        "                if sleep_time > 0:\n",
        "                    time.sleep(sleep_time) # Sleep for the remaining interval time\n",
        "\n",
        "        finally:\n",
        "            if self.sct:\n",
        "                self.sct.close()\n",
        "            logger.info(\"Screen capture stopped.\")\n",
        "\n",
        "    def stop(self):\n",
        "        self.running = False\n",
        "\n",
        "    @Slot(int)\n",
        "    def update_capture_interval(self, interval):\n",
        "        with QMutexLocker(self._lock):\n",
        "            if interval > 0:\n",
        "                logger.info(f\"Updating capture interval to {interval} ms\")\n",
        "                self._capture_interval_ms = interval\n",
        "            else:\n",
        "                 logger.warning(f\"Ignoring invalid capture interval: {interval} ms\")\n",
        "\n",
        "# --- Base Worker Thread for YOLO Models ---\n",
        "class BaseYoloWorker(QThread):\n",
        "    \"\"\"Base class for YOLO detection and pose estimation threads.\"\"\"\n",
        "    status_update = Signal(str)\n",
        "\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self.input_frame = None\n",
        "        self.frame_timestamp = 0.0\n",
        "        self.running = False\n",
        "        self._enabled = True\n",
        "        self._confidence_threshold = DEFAULT_CONFIDENCE_THRESHOLD\n",
        "        self._nms_threshold = DEFAULT_NMS_THRESHOLD # Used by object detection\n",
        "        self._frame_lock = QMutex()\n",
        "        self.device = None\n",
        "        self.processing_time_ms = 0.0\n",
        "\n",
        "    def load_model(self):\n",
        "        self.status_update.emit(f\"Loading model {self.model_name}...\")\n",
        "        logger.info(f\"Attempting to load YOLO model: {self.model_name}\")\n",
        "        try:\n",
        "            self.model = YOLO(self.model_name)\n",
        "            if torch.cuda.is_available():\n",
        "                self.device = torch.device('cuda')\n",
        "                self.model.to(self.device)\n",
        "                device_name = torch.cuda.get_device_name(0)\n",
        "                logger.info(f\"Model '{self.model_name}' loaded on GPU: {device_name}.\")\n",
        "                self.status_update.emit(f\"Model '{self.model_name}' loaded on GPU.\")\n",
        "            else:\n",
        "                self.device = torch.device('cpu')\n",
        "                self.model.to(self.device)\n",
        "                logger.info(f\"Model '{self.model_name}' loaded on CPU.\")\n",
        "                self.status_update.emit(f\"Model '{self.model_name}' loaded on CPU.\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Failed to load model '{self.model_name}': {e}\"\n",
        "            logger.error(error_msg, exc_info=True)\n",
        "            self.status_update.emit(error_msg)\n",
        "            self.model = None\n",
        "            self.device = None\n",
        "            return False\n",
        "\n",
        "    @Slot(np.ndarray, float)\n",
        "    def set_frame(self, frame, timestamp):\n",
        "        with QMutexLocker(self._frame_lock):\n",
        "            # Keep the latest frame and its timestamp\n",
        "            self.input_frame = frame.copy()\n",
        "            self.frame_timestamp = timestamp\n",
        "\n",
        "    def run(self):\n",
        "        if not self.load_model():\n",
        "            self.running = False\n",
        "            return\n",
        "\n",
        "        self.running = True\n",
        "        logger.info(f\"{self.__class__.__name__} thread started.\")\n",
        "        while self.running:\n",
        "            if self._enabled:\n",
        "                frame_to_process = None\n",
        "                timestamp_to_process = 0.0\n",
        "                with QMutexLocker(self._frame_lock):\n",
        "                    if self.input_frame is not None:\n",
        "                        frame_to_process = self.input_frame\n",
        "                        timestamp_to_process = self.frame_timestamp\n",
        "                        self.input_frame = None # Consume frame\n",
        "\n",
        "                if frame_to_process is not None and self.model is not None:\n",
        "                    start_time = time.perf_counter()\n",
        "                    try:\n",
        "                        # Perform inference on the correct device\n",
        "                        results = self.model(\n",
        "                            frame_to_process,\n",
        "                            conf=self._confidence_threshold,\n",
        "                            iou=self._nms_threshold, # NMS relevant for detection, ignored by pose? Check docs.\n",
        "                            verbose=False,\n",
        "                            device=self.device\n",
        "                        )\n",
        "                        # Process results (implemented in subclasses)\n",
        "                        self.process_results(results, timestamp_to_process)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"{self.__class__.__name__} error: {e}\", exc_info=True)\n",
        "                        self.status_update.emit(f\"{self.__class__.__name__} error: {e}\")\n",
        "                    finally:\n",
        "                         # Calculate processing time\n",
        "                        end_time = time.perf_counter()\n",
        "                        self.processing_time_ms = (end_time - start_time) * 1000\n",
        "\n",
        "                else:\n",
        "                    # No frame, sleep briefly\n",
        "                    self.msleep(5)\n",
        "            else:\n",
        "                # Disabled, sleep longer\n",
        "                self.msleep(50)\n",
        "\n",
        "        logger.info(f\"{self.__class__.__name__} thread stopped.\")\n",
        "        self.model = None\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def process_results(self, results, timestamp):\n",
        "        \"\"\"Placeholder: Subclasses must implement this to process model output.\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def stop(self):\n",
        "        self.running = False\n",
        "\n",
        "    @Slot(bool)\n",
        "    def set_enabled(self, enabled):\n",
        "        self._enabled = enabled\n",
        "        logger.info(f\"{self.__class__.__name__} {'enabled' if enabled else 'disabled'}\")\n",
        "        if not enabled:\n",
        "             # Clear potential pending frame when disabled\n",
        "             with QMutexLocker(self._frame_lock):\n",
        "                 self.input_frame = None\n",
        "\n",
        "    @Slot(float)\n",
        "    def update_confidence_threshold(self, threshold):\n",
        "        self._confidence_threshold = threshold\n",
        "        logger.info(f\"{self.__class__.__name__} confidence threshold updated to {threshold:.2f}\")\n",
        "\n",
        "    @Slot(float)\n",
        "    def update_nms_threshold(self, threshold):\n",
        "        # Only relevant for object detection, but keep slot for potential future use\n",
        "        self._nms_threshold = threshold\n",
        "        logger.info(f\"{self.__class__.__name__} NMS threshold updated to {threshold:.2f}\")\n",
        "\n",
        "    def get_processing_time(self):\n",
        "        return self.processing_time_ms\n",
        "\n",
        "# --- Detection Thread ---\n",
        "class DetectionThread(BaseYoloWorker):\n",
        "    \"\"\"Performs object detection using a YOLO model.\"\"\"\n",
        "    # Emits: list of (label, confidence, box_tuple), timestamp, processing_time_ms\n",
        "    detections_ready = Signal(list, float, float)\n",
        "\n",
        "    def __init__(self, model_name=OBJECT_DETECTION_MODEL):\n",
        "        super().__init__(model_name)\n",
        "        self._confidence_threshold = DEFAULT_CONFIDENCE_THRESHOLD # Reset specific default\n",
        "        self._nms_threshold = DEFAULT_NMS_THRESHOLD\n",
        "\n",
        "    def process_results(self, results, timestamp):\n",
        "        \"\"\"Processes YOLO object detection results.\"\"\"\n",
        "        detections = []\n",
        "        if results and results[0]:\n",
        "            boxes = results[0].boxes\n",
        "            for box in boxes:\n",
        "                # Move results to CPU for signal emission\n",
        "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                confidence = box.conf[0].cpu().numpy()\n",
        "                class_id = int(box.cls[0].cpu().numpy())\n",
        "                label = self.model.names.get(class_id, f\"ID:{class_id}\")\n",
        "                detections.append((label, float(confidence), (int(x1), int(y1), int(x2), int(y2))))\n",
        "\n",
        "        self.detections_ready.emit(detections, timestamp, self.get_processing_time())\n",
        "\n",
        "# --- Pose Estimation Thread ---\n",
        "class PoseEstimationThread(BaseYoloWorker):\n",
        "    \"\"\"Performs pose estimation using a YOLO pose model.\"\"\"\n",
        "    # Emits: list of (keypoints_array, box_tuple, confidence), timestamp, processing_time_ms\n",
        "    # keypoints_array is N x 3 (x, y, confidence) for N keypoints\n",
        "    poses_ready = Signal(list, float, float)\n",
        "\n",
        "    def __init__(self, model_name=POSE_ESTIMATION_MODEL):\n",
        "        super().__init__(model_name)\n",
        "        self._confidence_threshold = DEFAULT_POSE_CONFIDENCE_THRESHOLD # Use pose-specific threshold\n",
        "        # NMS is less critical/different for pose, set default but may not be used same way\n",
        "        self._nms_threshold = DEFAULT_NMS_THRESHOLD\n",
        "\n",
        "    def process_results(self, results, timestamp):\n",
        "        \"\"\"Processes YOLO pose estimation results.\"\"\"\n",
        "        poses = []\n",
        "        if results and results[0]:\n",
        "            keypoints_list = results[0].keypoints # Access keypoints directly\n",
        "            boxes = results[0].boxes # Get associated bounding boxes if needed\n",
        "\n",
        "            for i, kpts in enumerate(keypoints_list):\n",
        "                # kpts.xy are the coordinates, kpts.conf contains confidences\n",
        "                # Combine them into an N x 3 array (x, y, conf)\n",
        "                kpts_data_cpu = kpts.data[0].cpu().numpy() # Get tensor data, move to CPU\n",
        "\n",
        "                # Extract bounding box and overall confidence for this pose instance\n",
        "                box_data = boxes[i]\n",
        "                x1, y1, x2, y2 = box_data.xyxy[0].cpu().numpy()\n",
        "                pose_confidence = box_data.conf[0].cpu().numpy() # Overall confidence for the detected person/pose\n",
        "\n",
        "                # Filter keypoints by individual confidence if needed (optional)\n",
        "                # kpts_data_cpu[kpts_data_cpu[:, 2] < KEYPOINT_VISIBILITY_THRESHOLD] = [0, 0, 0] # Example filtering\n",
        "\n",
        "                poses.append((kpts_data_cpu, (int(x1), int(y1), int(x2), int(y2)), float(pose_confidence)))\n",
        "\n",
        "        self.poses_ready.emit(poses, timestamp, self.get_processing_time())\n",
        "\n",
        "# --- Optical Flow Thread (Remains largely unchanged, ensure set_enabled works) ---\n",
        "class OpticalFlowThread(QThread):\n",
        "    \"\"\"Calculates sparse optical flow (GPU accelerated if OpenCV CUDA is available).\"\"\"\n",
        "    flow_ready = Signal(list) # List of (start_point_tuple, end_point_tuple)\n",
        "    status_update = Signal(str)\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.running = False\n",
        "        self._enabled = False\n",
        "        self._frame_lock = QMutex()\n",
        "        self.current_frame = None\n",
        "        self.use_gpu = False\n",
        "        self.gpu_detector = None\n",
        "        self.gpu_lk_flow = None\n",
        "        self.prev_gray_gpu = None\n",
        "        self.prev_points_gpu = None\n",
        "        self.cpu_lk_params = dict(winSize=(21, 21), maxLevel=3, # Slightly larger window, more levels\n",
        "                                  criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "        self.cpu_feature_params = dict(maxCorners=100, qualityLevel=0.2, minDistance=7, blockSize=7) # Slightly lower quality level\n",
        "        self.prev_gray_cpu = None\n",
        "        self.prev_points_cpu = None\n",
        "        self.processing_time_ms = 0.0\n",
        "\n",
        "        # Check for OpenCV CUDA support (No changes needed here)\n",
        "        try:\n",
        "            if cv2.cuda.getCudaEnabledDeviceCount() > 0:\n",
        "                logger.info(\"OpenCV CUDA detected. Attempting to use GPU for Optical Flow.\")\n",
        "                # Ensure parameters match CPU version if desired\n",
        "                gpu_feature_params = self.cpu_feature_params.copy()\n",
        "                gpu_feature_params['qualityLevel'] = float(gpu_feature_params['qualityLevel']) # Ensure float type\n",
        "                gpu_feature_params['minDistance'] = float(gpu_feature_params['minDistance']) # Ensure float type\n",
        "\n",
        "                self.gpu_detector = cv2.cuda.createGoodFeaturesToTrackDetector(cv2.CV_8UC1, **gpu_feature_params)\n",
        "                self.gpu_lk_flow = cv2.cuda.SparsePyrLKOpticalFlow_create(winSize=self.cpu_lk_params['winSize'],\n",
        "                                                                           maxLevel=self.cpu_lk_params['maxLevel'],\n",
        "                                                                           iters=self.cpu_lk_params['criteria'][2])\n",
        "                self.use_gpu = True\n",
        "                logger.info(\"Successfully initialized GPU Optical Flow components.\")\n",
        "                self.status_update.emit(\"Optical Flow: Using GPU\")\n",
        "            else:\n",
        "                logger.info(\"No OpenCV CUDA devices found. Using CPU for Optical Flow.\")\n",
        "                self.status_update.emit(\"Optical Flow: Using CPU\")\n",
        "        except AttributeError:\n",
        "            logger.warning(\"OpenCV CUDA module not found or unavailable in this build. Using CPU for Optical Flow.\")\n",
        "            self.status_update.emit(\"Optical Flow: Using CPU (CUDA module unavailable)\")\n",
        "            self.use_gpu = False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing OpenCV CUDA components: {e}. Falling back to CPU.\", exc_info=True)\n",
        "            self.use_gpu = False\n",
        "            self.status_update.emit(\"Optical Flow: Error initializing GPU, using CPU\")\n",
        "\n",
        "\n",
        "    @Slot(np.ndarray, float) # Accept timestamp, though not directly used in flow calc\n",
        "    def set_frame(self, frame, timestamp):\n",
        "        with QMutexLocker(self._frame_lock):\n",
        "            self.current_frame = frame.copy()\n",
        "\n",
        "    def run(self):\n",
        "        self.running = True\n",
        "        logger.info(f\"Optical Flow thread started (GPU Enabled: {self.use_gpu}).\")\n",
        "        while self.running:\n",
        "            if self._enabled:\n",
        "                frame = None\n",
        "                with QMutexLocker(self._frame_lock):\n",
        "                    if self.current_frame is not None:\n",
        "                        frame = self.current_frame\n",
        "                        self.current_frame = None\n",
        "\n",
        "                if frame is not None:\n",
        "                    start_time = time.perf_counter()\n",
        "                    try:\n",
        "                        if self.use_gpu:\n",
        "                            self.run_gpu(frame)\n",
        "                        else:\n",
        "                            self.run_cpu(frame)\n",
        "                    except cv2.error as e:\n",
        "                         logger.error(f\"OpenCV error in Optical Flow: {e}\", exc_info=True)\n",
        "                         self.status_update.emit(f\"Optical Flow Error: {e}\")\n",
        "                         self.reset_state()\n",
        "                         self.msleep(100)\n",
        "                    except Exception as e:\n",
        "                         logger.error(f\"Unexpected error in Optical Flow: {e}\", exc_info=True)\n",
        "                         self.status_update.emit(f\"Optical Flow Error: {e}\")\n",
        "                         self.reset_state()\n",
        "                         self.msleep(100)\n",
        "                    finally:\n",
        "                        end_time = time.perf_counter()\n",
        "                        self.processing_time_ms = (end_time - start_time) * 1000\n",
        "                else:\n",
        "                     self.msleep(5)\n",
        "            else:\n",
        "                # Reset state only if it was previously enabled\n",
        "                if self.prev_gray_cpu is not None or self.prev_gray_gpu is not None:\n",
        "                     self.reset_state()\n",
        "                self.msleep(50)\n",
        "\n",
        "        logger.info(\"Optical Flow thread stopped.\")\n",
        "        self.reset_state()\n",
        "\n",
        "    def run_gpu(self, frame):\n",
        "        frame_gpu = cv2.cuda_GpuMat()\n",
        "        frame_gpu.upload(frame)\n",
        "        gray_gpu = cv2.cuda.cvtColor(frame_gpu, cv2.COLOR_BGR2GRAY)\n",
        "        flow_vectors = []\n",
        "        points_to_track = False\n",
        "\n",
        "        if self.prev_gray_gpu is not None and self.prev_points_gpu is not None and not self.prev_points_gpu.empty():\n",
        "            points_to_track = True\n",
        "            # Ensure prev_points_gpu is float32 (should be from detector, but double-check)\n",
        "            if self.prev_points_gpu.type() != cv2.CV_32FC2:\n",
        "                 # This indicates an issue, log warning and attempt conversion\n",
        "                 logger.warning(\"prev_points_gpu is not CV_32FC2, attempting conversion.\")\n",
        "                 temp_cpu = self.prev_points_gpu.download().astype(np.float32)\n",
        "                 temp_gpu = cv2.cuda_GpuMat()\n",
        "                 temp_gpu.upload(temp_cpu)\n",
        "                 self.prev_points_gpu = temp_gpu\n",
        "                 if self.prev_points_gpu.empty(): # Check if conversion failed\n",
        "                      points_to_track = False\n",
        "                      logger.error(\"Failed to convert prev_points_gpu to CV_32FC2.\")\n",
        "\n",
        "\n",
        "            if points_to_track:\n",
        "                next_points_gpu, status_gpu, err_gpu = self.gpu_lk_flow.calc(self.prev_gray_gpu, gray_gpu, self.prev_points_gpu, None)\n",
        "\n",
        "                if next_points_gpu is not None and status_gpu is not None:\n",
        "                    status = status_gpu.download().flatten()\n",
        "                    # Filter based on status BEFORE downloading points for efficiency\n",
        "                    prev_points_gpu_filtered = self.prev_points_gpu[status == 1]\n",
        "                    next_points_gpu_filtered = next_points_gpu[status == 1]\n",
        "\n",
        "\n",
        "                    if not next_points_gpu_filtered.empty() and not prev_points_gpu_filtered.empty():\n",
        "                        good_new = next_points_gpu_filtered.download().reshape(-1, 2)\n",
        "                        good_old = prev_points_gpu_filtered.download().reshape(-1, 2)\n",
        "\n",
        "                        flow_vectors = [(tuple(map(int, p)), tuple(map(int, q))) for p, q in zip(good_old, good_new)]\n",
        "                        self.prev_points_gpu = next_points_gpu_filtered # Update with the successfully tracked points (already filtered)\n",
        "                    else:\n",
        "                        points_to_track = False # Lost all points\n",
        "                        self.prev_points_gpu = None # Reset points\n",
        "                else:\n",
        "                    points_to_track = False\n",
        "                    self.prev_points_gpu = None\n",
        "\n",
        "        # Detect new features if needed\n",
        "        # Only detect if we have too few points or lost track\n",
        "        detect_new = False\n",
        "        if not points_to_track:\n",
        "            detect_new = True\n",
        "        elif self.prev_points_gpu is not None and self.prev_points_gpu.rows() < self.cpu_feature_params['maxCorners'] * 0.5: # Redetect if points drop below 50%\n",
        "             detect_new = True\n",
        "\n",
        "\n",
        "        if detect_new:\n",
        "            # logger.debug(\"Detecting new features (GPU)...\") # Optional debug log\n",
        "            # Mask is None here, could potentially mask out areas with existing detections\n",
        "            detected_points_gpu_mat = self.gpu_detector.detect(gray_gpu, None)\n",
        "\n",
        "            if detected_points_gpu_mat is not None and not detected_points_gpu_mat.empty():\n",
        "                # Convert detected points (keypoints) to the format needed by LK (float32 points)\n",
        "                # Keypoints might be N x 1 GpuMat of KeyPoint objects, need conversion\n",
        "                # Or N x 1 x 2 float32 points directly, depending on OpenCV version/detector\n",
        "                # Assuming N x 1 x 2 float32 format here based on typical LK input needs\n",
        "                # If it's KeyPoint objects, conversion is needed:\n",
        "                # keypoints_cpu = detected_points_gpu_mat.download() # Download if KeyPoint objects\n",
        "                # points_cpu = cv2.KeyPoint_convert(keypoints_cpu) # Convert KeyPoints to points\n",
        "                # points_gpu_float32 = cv2.cuda_GpuMat()\n",
        "                # points_gpu_float32.upload(points_cpu.astype(np.float32))\n",
        "                # self.prev_points_gpu = points_gpu_float32.reshape(-1, 1, 2) # Ensure shape\n",
        "\n",
        "                # Simpler approach if detector directly gives points:\n",
        "                 self.prev_points_gpu = detected_points_gpu_mat.reshape(-1, 1, 2) # Ensure shape\n",
        "                 if self.prev_points_gpu.type() != cv2.CV_32FC2:\n",
        "                     # Convert if necessary\n",
        "                     temp_cpu = self.prev_points_gpu.download().astype(np.float32)\n",
        "                     temp_gpu = cv2.cuda_GpuMat()\n",
        "                     temp_gpu.upload(temp_cpu)\n",
        "                     self.prev_points_gpu = temp_gpu\n",
        "\n",
        "\n",
        "                 # logger.debug(f\"Detected {self.prev_points_gpu.rows()} new features (GPU).\")\n",
        "            else:\n",
        "                self.prev_points_gpu = None # No features found\n",
        "                # logger.debug(\"No new features detected (GPU).\")\n",
        "\n",
        "\n",
        "        self.prev_gray_gpu = gray_gpu\n",
        "        if flow_vectors:\n",
        "            self.flow_ready.emit(flow_vectors)\n",
        "\n",
        "\n",
        "    def run_cpu(self, frame):\n",
        "        gray_cpu = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        flow_vectors = []\n",
        "        points_to_track = False\n",
        "\n",
        "        if self.prev_gray_cpu is not None and self.prev_points_cpu is not None and len(self.prev_points_cpu) > 0:\n",
        "            points_to_track = True\n",
        "            next_points_cpu, status, err = cv2.calcOpticalFlowPyrLK(\n",
        "                self.prev_gray_cpu, gray_cpu, self.prev_points_cpu, None, **self.cpu_lk_params\n",
        "            )\n",
        "\n",
        "            if next_points_cpu is not None and status is not None:\n",
        "                good_new = next_points_cpu[status == 1]\n",
        "                good_old = self.prev_points_cpu[status == 1]\n",
        "\n",
        "                if len(good_new) > 0:\n",
        "                     flow_vectors = [(tuple(map(int, p)), tuple(map(int, q))) for p, q in zip(good_old, good_new)]\n",
        "                     self.prev_points_cpu = good_new.reshape(-1, 1, 2) # Update with tracked points\n",
        "                else:\n",
        "                    points_to_track = False # Lost all points\n",
        "                    self.prev_points_cpu = None # Reset points\n",
        "            else:\n",
        "                points_to_track = False\n",
        "                self.prev_points_cpu = None\n",
        "\n",
        "\n",
        "        # Detect new features if needed\n",
        "        detect_new = False\n",
        "        if not points_to_track:\n",
        "            detect_new = True\n",
        "        elif self.prev_points_cpu is not None and len(self.prev_points_cpu) < self.cpu_feature_params['maxCorners'] * 0.5:\n",
        "            detect_new = True\n",
        "\n",
        "        if detect_new:\n",
        "            # logger.debug(\"Detecting new features (CPU)...\")\n",
        "            self.prev_points_cpu = cv2.goodFeaturesToTrack(gray_cpu, mask=None, **self.cpu_feature_params)\n",
        "            # if self.prev_points_cpu is not None:\n",
        "            #      logger.debug(f\"Detected {len(self.prev_points_cpu)} new features (CPU).\")\n",
        "            # else:\n",
        "            #      logger.debug(\"No new features detected (CPU).\")\n",
        "\n",
        "\n",
        "        self.prev_gray_cpu = gray_cpu.copy()\n",
        "        if flow_vectors:\n",
        "            self.flow_ready.emit(flow_vectors)\n",
        "\n",
        "\n",
        "    def stop(self):\n",
        "        self.running = False\n",
        "\n",
        "    def reset_state(self):\n",
        "        \"\"\"Resets the internal state (both CPU and GPU).\"\"\"\n",
        "        self.prev_gray_gpu = None\n",
        "        self.prev_points_gpu = None\n",
        "        self.prev_gray_cpu = None\n",
        "        self.prev_points_cpu = None\n",
        "        # logger.debug(\"Optical flow state reset.\") # Optional debug log\n",
        "\n",
        "    @Slot(bool)\n",
        "    def set_enabled(self, enabled):\n",
        "        if self._enabled != enabled:\n",
        "            self._enabled = enabled\n",
        "            logger.info(f\"Optical Flow {'enabled' if enabled else 'disabled'}\")\n",
        "            if not enabled:\n",
        "                # Reset state when disabling to clear old points/frames\n",
        "                self.reset_state()\n",
        "\n",
        "    def get_processing_time(self):\n",
        "        return self.processing_time_ms\n",
        "\n",
        "\n",
        "# --- Depth Estimation Thread (Remains largely unchanged, ensure set_enabled works) ---\n",
        "class DepthEstimationThread(QThread):\n",
        "    \"\"\"Performs depth estimation using a MiDaS model (GPU if available).\"\"\"\n",
        "    depth_ready = Signal(np.ndarray) # Emits normalized depth map (0-1, CPU numpy array)\n",
        "    status_update = Signal(str)\n",
        "\n",
        "    def __init__(self, model_type=\"MiDaS_small\"):\n",
        "        super().__init__()\n",
        "        self.model_type = model_type\n",
        "        self.model = None\n",
        "        self.transform = None\n",
        "        self.running = False\n",
        "        self._enabled = False\n",
        "        self._frame_lock = QMutex()\n",
        "        self.current_frame = None\n",
        "        self.device = None\n",
        "        self.processing_time_ms = 0.0\n",
        "\n",
        "    def load_model(self):\n",
        "        self.status_update.emit(f\"Loading MiDaS model: {self.model_type}...\")\n",
        "        logger.info(f\"Attempting to load MiDaS model: {self.model_type}\")\n",
        "        try:\n",
        "            # Determine device\n",
        "            if torch.cuda.is_available():\n",
        "                self.device = torch.device(\"cuda\")\n",
        "                logger.info(\"Using GPU for MiDaS.\")\n",
        "            else:\n",
        "                self.device = torch.device(\"cpu\")\n",
        "                logger.info(\"Using CPU for MiDaS.\")\n",
        "\n",
        "            # Load model and transform from PyTorch Hub\n",
        "            self.model = torch.hub.load(\"intel-isl/MiDaS\", self.model_type, trust_repo=True)\n",
        "            midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\", trust_repo=True)\n",
        "\n",
        "            if self.model_type == \"MiDaS_small\":\n",
        "                 self.transform = midas_transforms.small_transform\n",
        "            elif \"dpt_large\" in self.model_type.lower() or \"dpt_hybrid\" in self.model_type.lower():\n",
        "                 self.transform = midas_transforms.dpt_transform\n",
        "            else: # Default or other MiDaS v2.1 models\n",
        "                 self.transform = midas_transforms.dpt_transform # DPT transform often works well\n",
        "\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval() # Set model to evaluation mode\n",
        "            logger.info(f\"MiDaS model '{self.model_type}' loaded on {self.device}.\")\n",
        "            self.status_update.emit(f\"MiDaS model '{self.model_type}' loaded on {self.device}.\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Failed to load MiDaS model '{self.model_type}': {e}\"\n",
        "            logger.error(error_msg, exc_info=True)\n",
        "            self.status_update.emit(error_msg)\n",
        "            self.model = None\n",
        "            self.transform = None\n",
        "            self.device = None\n",
        "            return False\n",
        "\n",
        "    @Slot(np.ndarray, float) # Accept timestamp, though not directly used\n",
        "    def set_frame(self, frame, timestamp):\n",
        "        with QMutexLocker(self._frame_lock):\n",
        "            self.current_frame = frame.copy()\n",
        "\n",
        "    def run(self):\n",
        "        if not self.load_model():\n",
        "            self.running = False\n",
        "            return\n",
        "\n",
        "        self.running = True\n",
        "        logger.info(\"Depth Estimation thread started.\")\n",
        "        while self.running:\n",
        "            if self._enabled:\n",
        "                frame_to_process = None\n",
        "                with QMutexLocker(self._frame_lock):\n",
        "                    if self.current_frame is not None:\n",
        "                        frame_to_process = self.current_frame\n",
        "                        self.current_frame = None\n",
        "\n",
        "                if frame_to_process is not None and self.model is not None and self.transform is not None:\n",
        "                    start_time = time.perf_counter()\n",
        "                    try:\n",
        "                        # Preprocess frame (runs on CPU)\n",
        "                        # Ensure input is RGB for MiDaS transforms\n",
        "                        if frame_to_process.shape[2] == 3: # BGR\n",
        "                            img_rgb = cv2.cvtColor(frame_to_process, cv2.COLOR_BGR2RGB)\n",
        "                        else: # Grayscale or other? Skip if not BGR/RGB\n",
        "                             logger.warning(\"Depth estimation requires BGR input frame.\")\n",
        "                             continue\n",
        "\n",
        "                        input_batch = self.transform(img_rgb).to(self.device) # Transform and move to device\n",
        "\n",
        "                        with torch.no_grad(): # Inference without gradient calculation\n",
        "                            prediction = self.model(input_batch)\n",
        "\n",
        "                            # Resize prediction to original image size\n",
        "                            prediction = torch.nn.functional.interpolate(\n",
        "                                prediction.unsqueeze(1),\n",
        "                                size=img_rgb.shape[:2], # Use original RGB frame height, width\n",
        "                                mode=\"bicubic\",        # Smoother interpolation\n",
        "                                align_corners=False,\n",
        "                            ).squeeze()\n",
        "\n",
        "                        # Move depth map to CPU and normalize\n",
        "                        depth_map = prediction.cpu().numpy()\n",
        "                        # Normalize to 0-1 range for visualization\n",
        "                        normalized_depth = cv2.normalize(depth_map, None, 0, 1, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "\n",
        "                        self.depth_ready.emit(normalized_depth)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Depth estimation error: {e}\", exc_info=True)\n",
        "                        self.status_update.emit(f\"Depth estimation error: {e}\")\n",
        "                    finally:\n",
        "                        end_time = time.perf_counter()\n",
        "                        self.processing_time_ms = (end_time - start_time) * 1000\n",
        "                else:\n",
        "                    self.msleep(5)\n",
        "            else:\n",
        "                 # Clear potential pending frame when disabled\n",
        "                 with QMutexLocker(self._frame_lock):\n",
        "                      self.current_frame = None\n",
        "                 self.msleep(50)\n",
        "\n",
        "        logger.info(\"Depth Estimation thread stopped.\")\n",
        "        self.model = None\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def stop(self):\n",
        "        self.running = False\n",
        "\n",
        "    @Slot(bool)\n",
        "    def set_enabled(self, enabled):\n",
        "        if self._enabled != enabled:\n",
        "            self._enabled = enabled\n",
        "            logger.info(f\"Depth Estimation {'enabled' if enabled else 'disabled'}\")\n",
        "            if not enabled:\n",
        "                 # Clear potential pending frame when disabled\n",
        "                 with QMutexLocker(self._frame_lock):\n",
        "                      self.current_frame = None\n",
        "\n",
        "    def get_processing_time(self):\n",
        "        return self.processing_time_ms\n",
        "\n",
        "\n",
        "# --- Overlay Widget ---\n",
        "class OverlayWidget(QWidget):\n",
        "    \"\"\"Displays the transparent overlay with detections, paths, etc.\"\"\"\n",
        "    def __init__(self, monitor_rect, parent=None):\n",
        "        super().__init__(parent)\n",
        "        # Make window transparent, frameless, and stay on top\n",
        "        self.setWindowFlags(Qt.FramelessWindowHint | Qt.WindowStaysOnTopHint | Qt.Tool)\n",
        "        self.setAttribute(Qt.WA_TranslucentBackground)\n",
        "        self.setAttribute(Qt.WA_NoSystemBackground, True)\n",
        "        self.setAttribute(Qt.WA_PaintOnScreen) # May improve performance\n",
        "\n",
        "        # Geometry needs to match the captured monitor exactly\n",
        "        self.monitor_rect = monitor_rect\n",
        "        self.setGeometry(monitor_rect.left(), monitor_rect.top(), monitor_rect.width(), monitor_rect.height())\n",
        "\n",
        "        # --- Font Setup ---\n",
        "        font_id = QFontDatabase.addApplicationFont(\"PressStart2P-Regular.ttf\") # Ensure font file is present\n",
        "        if font_id != -1:\n",
        "            font_families = QFontDatabase.applicationFontFamilies(font_id)\n",
        "            if font_families:\n",
        "                self.hud_font_name = font_families[0]\n",
        "                logger.info(f\"Loaded font: {self.hud_font_name}\")\n",
        "            else:\n",
        "                self.hud_font_name = FALLBACK_FONT\n",
        "                logger.warning(\"Could not get font family name, using fallback.\")\n",
        "        else:\n",
        "            self.hud_font_name = FALLBACK_FONT\n",
        "            logger.warning(\"Could not load font 'PressStart2P-Regular.ttf', using fallback.\")\n",
        "\n",
        "        self.font_small = QFont(self.hud_font_name, FONT_SIZE_SMALL)\n",
        "        self.font_medium = QFont(self.hud_font_name, FONT_SIZE_MEDIUM)\n",
        "\n",
        "        # --- Data Storage ---\n",
        "        self.detections = []\n",
        "        self.poses = []\n",
        "        self.flow_vectors = []\n",
        "        self.depth_map = None\n",
        "        self.hud_texts = {}\n",
        "        self.target_history = collections.deque(maxlen=PATH_HISTORY_LENGTH)\n",
        "        self.smoothed_target_center = None # QPointF for smoothed crosshair\n",
        "        self.smoothed_velocity = QPointF(0, 0) # Smoothed velocity vector\n",
        "\n",
        "        # --- Drawing Flags ---\n",
        "        self.show_boxes = True\n",
        "        self.show_labels = True\n",
        "        self.show_paths = True\n",
        "        self.show_trajectory = True\n",
        "        self.show_crosshair = True\n",
        "        self.show_poses = True # Flag for drawing poses\n",
        "        self.show_flow = False\n",
        "        self.show_depth = False\n",
        "        self.show_hud = True\n",
        "\n",
        "        # --- Scanline Effect ---\n",
        "        self.scanline_y = 0\n",
        "        self.scanline_timer = QTimer(self)\n",
        "        self.scanline_timer.timeout.connect(self.update_scanline)\n",
        "        self.scanline_timer.start(SCANLINE_SPEED_MS)\n",
        "\n",
        "        # --- HUD Update Timer ---\n",
        "        self.hud_update_timer = QTimer(self)\n",
        "        self.hud_update_timer.timeout.connect(self.update) # Trigger repaint for HUD updates\n",
        "        self.hud_update_timer.start(UPDATE_INTERVAL_MS)\n",
        "\n",
        "        # --- Performance Metrics ---\n",
        "        self.capture_fps = 0.0\n",
        "        self.detection_fps = 0.0\n",
        "        self.pose_fps = 0.0\n",
        "        self.flow_fps = 0.0\n",
        "        self.depth_fps = 0.0\n",
        "        self.last_capture_time = time.perf_counter()\n",
        "        self.last_detection_time = time.perf_counter()\n",
        "        self.last_pose_time = time.perf_counter()\n",
        "        self.last_flow_time = time.perf_counter()\n",
        "        self.last_depth_time = time.perf_counter()\n",
        "        self.detection_proc_time = 0.0\n",
        "        self.pose_proc_time = 0.0\n",
        "        self.flow_proc_time = 0.0\n",
        "        self.depth_proc_time = 0.0\n",
        "\n",
        "\n",
        "    def update_scanline(self):\n",
        "        \"\"\"Updates the position of the scanline effect.\"\"\"\n",
        "        self.scanline_y = (self.scanline_y + 5) % self.height()\n",
        "        self.update() # Trigger repaint\n",
        "\n",
        "    @Slot(list, float, float)\n",
        "    def update_detections(self, detections, timestamp, proc_time):\n",
        "        \"\"\"Receives detection results from the detection thread.\"\"\"\n",
        "        self.detections = detections\n",
        "        self.detection_proc_time = proc_time\n",
        "\n",
        "        # --- Target Selection and Smoothing ---\n",
        "        # Example: Target the largest detected \"person\" box\n",
        "        target_box = None\n",
        "        max_area = 0\n",
        "        person_detections = [d for d in detections if d[0] == 'person'] # Filter for persons\n",
        "\n",
        "        if person_detections:\n",
        "            for label, conf, box in person_detections:\n",
        "                x1, y1, x2, y2 = box\n",
        "                area = (x2 - x1) * (y2 - y1)\n",
        "                if area > max_area:\n",
        "                    max_area = area\n",
        "                    target_box = box\n",
        "\n",
        "        # Calculate raw center\n",
        "        raw_target_center = None\n",
        "        if target_box:\n",
        "            x1, y1, x2, y2 = target_box\n",
        "            raw_target_center = QPointF((x1 + x2) / 2, (y1 + y2) / 2)\n",
        "\n",
        "        # --- Exponential Moving Average (EMA) for Smoothing ---\n",
        "        if raw_target_center:\n",
        "            if self.smoothed_target_center is None:\n",
        "                # Initialize smoothing on the first valid detection\n",
        "                self.smoothed_target_center = raw_target_center\n",
        "            else:\n",
        "                # Apply EMA: smooth = alpha * new + (1 - alpha) * old\n",
        "                self.smoothed_target_center = (TARGET_CENTER_SMOOTHING_FACTOR * raw_target_center +\n",
        "                                              (1.0 - TARGET_CENTER_SMOOTHING_FACTOR) * self.smoothed_target_center)\n",
        "\n",
        "            # --- Update Target History and Velocity ---\n",
        "            current_time = timestamp # Use the timestamp from the detection result\n",
        "            self.target_history.append((current_time, self.smoothed_target_center)) # Store smoothed center\n",
        "\n",
        "            # Calculate smoothed velocity for trajectory prediction\n",
        "            if len(self.target_history) >= 2:\n",
        "                 # Use last few points for velocity calculation\n",
        "                 points_to_use = min(TRAJECTORY_PREDICTION_POINTS, len(self.target_history))\n",
        "                 recent_points = list(self.target_history)[-points_to_use:]\n",
        "\n",
        "                 if len(recent_points) >= 2:\n",
        "                      # Calculate average velocity over the recent points\n",
        "                      total_dx = 0\n",
        "                      total_dy = 0\n",
        "                      total_dt = 0\n",
        "                      for i in range(len(recent_points) - 1):\n",
        "                           t1, p1 = recent_points[i]\n",
        "                           t2, p2 = recent_points[i+1]\n",
        "                           dt = t2 - t1\n",
        "                           if dt > 1e-6: # Avoid division by zero\n",
        "                                total_dx += (p2.x() - p1.x())\n",
        "                                total_dy += (p2.y() - p1.y())\n",
        "                                total_dt += dt\n",
        "\n",
        "                      if total_dt > 1e-6:\n",
        "                           raw_velocity = QPointF((total_dx / total_dt), (total_dy / total_dt))\n",
        "                           # Smooth the velocity vector itself using EMA\n",
        "                           self.smoothed_velocity = (TRAJECTORY_SMOOTHING_FACTOR * raw_velocity +\n",
        "                                                     (1.0 - TRAJECTORY_SMOOTHING_FACTOR) * self.smoothed_velocity)\n",
        "\n",
        "\n",
        "        else:\n",
        "            # No target detected this frame, maybe slowly decay velocity?\n",
        "            # Or keep last known velocity? For now, keep it.\n",
        "            # self.smoothed_target_center = None # Optional: Clear smoothed if no raw target\n",
        "            pass # Keep last smoothed position if no new raw target\n",
        "\n",
        "\n",
        "        # --- FPS Calculation ---\n",
        "        now = time.perf_counter()\n",
        "        time_diff = now - self.last_detection_time\n",
        "        if time_diff > 0:\n",
        "            self.detection_fps = 1.0 / time_diff\n",
        "        self.last_detection_time = now\n",
        "\n",
        "        self.update() # Trigger repaint\n",
        "\n",
        "    @Slot(list, float, float)\n",
        "    def update_poses(self, poses, timestamp, proc_time):\n",
        "        \"\"\"Receives pose estimation results.\"\"\"\n",
        "        self.poses = poses\n",
        "        self.pose_proc_time = proc_time\n",
        "        # --- FPS Calculation ---\n",
        "        now = time.perf_counter()\n",
        "        time_diff = now - self.last_pose_time\n",
        "        if time_diff > 0:\n",
        "            self.pose_fps = 1.0 / time_diff\n",
        "        self.last_pose_time = now\n",
        "        if self.show_poses: # Only repaint if poses are visible\n",
        "             self.update()\n",
        "\n",
        "    @Slot(list)\n",
        "    def update_flow(self, flow_vectors):\n",
        "        \"\"\"Receives optical flow results.\"\"\"\n",
        "        self.flow_vectors = flow_vectors\n",
        "        # --- FPS Calculation (Approximate based on signal arrival) ---\n",
        "        now = time.perf_counter()\n",
        "        time_diff = now - self.last_flow_time\n",
        "        if time_diff > 0:\n",
        "            self.flow_fps = 1.0 / time_diff\n",
        "        self.last_flow_time = now\n",
        "        if self.show_flow: # Only repaint if flow is visible\n",
        "            self.update()\n",
        "\n",
        "    @Slot(np.ndarray)\n",
        "    def update_depth(self, depth_map):\n",
        "        \"\"\"Receives depth estimation results.\"\"\"\n",
        "        self.depth_map = depth_map\n",
        "        # --- FPS Calculation (Approximate based on signal arrival) ---\n",
        "        now = time.perf_counter()\n",
        "        time_diff = now - self.last_depth_time\n",
        "        if time_diff > 0:\n",
        "            self.depth_fps = 1.0 / time_diff\n",
        "        self.last_depth_time = now\n",
        "        if self.show_depth: # Only repaint if depth is visible\n",
        "            self.update()\n",
        "\n",
        "    @Slot(dict)\n",
        "    def update_hud(self, hud_data):\n",
        "        \"\"\"Receives text data for the HUD.\"\"\"\n",
        "        self.hud_texts.update(hud_data)\n",
        "        # No repaint needed here, hud_update_timer handles it\n",
        "\n",
        "    @Slot(float)\n",
        "    def update_capture_fps(self, fps):\n",
        "        \"\"\"Receives capture FPS.\"\"\"\n",
        "        self.capture_fps = fps\n",
        "\n",
        "    # --- Toggling drawing elements ---\n",
        "    @Slot(bool)\n",
        "    def toggle_boxes(self, show): self.show_boxes = show; self.update()\n",
        "    @Slot(bool)\n",
        "    def toggle_labels(self, show): self.show_labels = show; self.update()\n",
        "    @Slot(bool)\n",
        "    def toggle_paths(self, show): self.show_paths = show; self.update()\n",
        "    @Slot(bool)\n",
        "    def toggle_trajectory(self, show): self.show_trajectory = show; self.update()\n",
        "    @Slot(bool)\n",
        "    def toggle_crosshair(self, show): self.show_crosshair = show; self.update()\n",
        "    @Slot(bool)\n",
        "    def toggle_poses(self, show): self.show_poses = show; self.update() # Slot for pose visibility\n",
        "    @Slot(bool)\n",
        "    def toggle_flow(self, show): self.show_flow = show; self.update()\n",
        "    @Slot(bool)\n",
        "    def toggle_depth(self, show): self.show_depth = show; self.update()\n",
        "    @Slot(bool)\n",
        "    def toggle_hud(self, show): self.show_hud = show; self.update()\n",
        "\n",
        "\n",
        "    def paintEvent(self, event):\n",
        "        \"\"\"Draws all overlay elements.\"\"\"\n",
        "        painter = QPainter(self)\n",
        "        painter.setRenderHint(QPainter.Antialiasing)\n",
        "\n",
        "        # --- Clear Background (Important for transparency) ---\n",
        "        painter.fillRect(self.rect(), Qt.transparent)\n",
        "\n",
        "        # --- 1. Depth Map (Draw first, as background) ---\n",
        "        if self.show_depth and self.depth_map is not None:\n",
        "            try:\n",
        "                # Convert normalized float32 depth map to 8-bit grayscale QImage\n",
        "                depth_8bit = (self.depth_map * 255).astype(np.uint8)\n",
        "                h, w = depth_8bit.shape\n",
        "                q_image = QImage(depth_8bit.data, w, h, w, QImage.Format_Grayscale8)\n",
        "                # Create QPixmap from QImage\n",
        "                pixmap = QPixmap.fromImage(q_image)\n",
        "                # Scale pixmap to fit widget size while maintaining aspect ratio\n",
        "                scaled_pixmap = pixmap.scaled(self.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation)\n",
        "                # Center the pixmap\n",
        "                x_offset = (self.width() - scaled_pixmap.width()) / 2\n",
        "                y_offset = (self.height() - scaled_pixmap.height()) / 2\n",
        "                painter.drawPixmap(x_offset, y_offset, scaled_pixmap)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error drawing depth map: {e}\", exc_info=True)\n",
        "\n",
        "\n",
        "        # --- 2. Scanline Effect ---\n",
        "        painter.setPen(QPen(QColor(0, 255, 0, 50), 2)) # Semi-transparent green line\n",
        "        painter.drawLine(0, self.scanline_y, self.width(), self.scanline_y)\n",
        "\n",
        "        # --- 3. Optical Flow Vectors ---\n",
        "        if self.show_flow and self.flow_vectors:\n",
        "            painter.setPen(QPen(YELLOW_COLOR, 1))\n",
        "            for start_pt, end_pt in self.flow_vectors:\n",
        "                painter.drawLine(QPoint(*start_pt), QPoint(*end_pt))\n",
        "                # Optionally draw points\n",
        "                # painter.drawPoint(QPoint(*end_pt))\n",
        "\n",
        "\n",
        "        # --- 4. Detections (Boxes and Labels) ---\n",
        "        if self.detections:\n",
        "            painter.setFont(self.font_small)\n",
        "            for label, confidence, box in self.detections:\n",
        "                x1, y1, x2, y2 = box\n",
        "                box_width = x2 - x1\n",
        "                box_height = y2 - y1\n",
        "\n",
        "                if self.show_boxes:\n",
        "                    painter.setPen(QPen(GREEN_COLOR, 2))\n",
        "                    painter.drawRect(x1, y1, box_width, box_height)\n",
        "\n",
        "                if self.show_labels:\n",
        "                    painter.setPen(GREEN_COLOR) # Pen for text outline (optional)\n",
        "                    painter.setBrush(GREEN_COLOR) # Brush for text fill\n",
        "                    text = f\"{label} ({confidence:.2f})\"\n",
        "                    # Simple text background\n",
        "                    # metrics = painter.fontMetrics()\n",
        "                    # text_width = metrics.horizontalAdvance(text)\n",
        "                    # text_height = metrics.height()\n",
        "                    # painter.fillRect(x1, y1 - text_height, text_width + 2, text_height, QColor(0,0,0,150))\n",
        "                    painter.drawText(x1 + 2, y1 - 5, text) # Position label above box\n",
        "\n",
        "\n",
        "        # --- 5. Pose Estimation Skeletons ---\n",
        "        if self.show_poses and self.poses:\n",
        "             painter.setPen(QPen(CYAN_COLOR, 2)) # Default color for keypoints\n",
        "             for keypoints_data, box, pose_conf in self.poses:\n",
        "                  # keypoints_data is N x 3 (x, y, confidence)\n",
        "                  num_keypoints = keypoints_data.shape[0]\n",
        "                  points = [] # Store QPointF for drawing connections\n",
        "                  # Draw Keypoints\n",
        "                  for i in range(num_keypoints):\n",
        "                       x, y, conf = keypoints_data[i]\n",
        "                       if conf > DEFAULT_POSE_CONFIDENCE_THRESHOLD: # Only draw visible keypoints\n",
        "                            pt = QPointF(x, y)\n",
        "                            points.append((i, pt)) # Store index and point\n",
        "                            painter.setBrush(POSE_COLORS[i % len(POSE_COLORS)]) # Cycle through colors\n",
        "                            painter.setPen(Qt.NoPen) # No outline for points\n",
        "                            painter.drawEllipse(pt, 3, 3) # Draw small circle for keypoint\n",
        "                       else:\n",
        "                            points.append((i, None)) # Mark invisible points\n",
        "\n",
        "                  # Draw Connections (Skeleton)\n",
        "                  painter.setPen(QPen(MAGENTA_COLOR, 1)) # Color for lines\n",
        "                  points_dict = {idx: pt for idx, pt in points if pt is not None} # Quick lookup for visible points\n",
        "                  for i, (start_idx, end_idx) in enumerate(POSE_CONNECTIONS):\n",
        "                       if start_idx in points_dict and end_idx in points_dict:\n",
        "                            pt1 = points_dict[start_idx]\n",
        "                            pt2 = points_dict[end_idx]\n",
        "                            # Optional: Color lines differently\n",
        "                            # painter.setPen(QPen(POSE_COLORS[i % len(POSE_COLORS)], 1))\n",
        "                            painter.drawLine(pt1, pt2)\n",
        "\n",
        "\n",
        "        # --- 6. Target Path History ---\n",
        "        if self.show_paths and len(self.target_history) > 1:\n",
        "            painter.setPen(QPen(QColor(255, 165, 0, 180), 1)) # Orange, semi-transparent\n",
        "            path_points = [p for t, p in self.target_history]\n",
        "            poly = QPolygonF(path_points)\n",
        "            painter.drawPolyline(poly)\n",
        "\n",
        "\n",
        "        # --- 7. Predicted Trajectory ---\n",
        "        if self.show_trajectory and self.smoothed_target_center and len(self.target_history) >= 2:\n",
        "            painter.setPen(QPen(QColor(255, 0, 255, 200), 2, Qt.DashLine)) # Magenta dashed line\n",
        "            # Predict future position based on smoothed velocity\n",
        "            start_point = self.smoothed_target_center\n",
        "            # Predict N steps into the future based on smoothed velocity and duration\n",
        "            # Note: This is a simple linear prediction. More complex models exist.\n",
        "            end_point = start_point + self.smoothed_velocity * TRAJECTORY_PREDICTION_DURATION\n",
        "            painter.drawLine(start_point, end_point)\n",
        "\n",
        "\n",
        "        # --- 8. Smoothed Crosshair ---\n",
        "        if self.show_crosshair and self.smoothed_target_center:\n",
        "            painter.setPen(QPen(RED_COLOR, 1))\n",
        "            # Draw crosshair at the smoothed target center\n",
        "            cx = int(self.smoothed_target_center.x())\n",
        "            cy = int(self.smoothed_target_center.y())\n",
        "            size = 10 # Size of the crosshair lines\n",
        "            painter.drawLine(cx - size, cy, cx + size, cy) # Horizontal line\n",
        "            painter.drawLine(cx, cy - size, cx, cy + size) # Vertical line\n",
        "            # Optional: Draw circle at center\n",
        "            # painter.drawEllipse(self.smoothed_target_center, 3, 3)\n",
        "\n",
        "            # --- Display Crosshair Calculation Info (Example in HUD instead) ---\n",
        "            # You could draw text near the crosshair, but HUD is cleaner:\n",
        "            # painter.setFont(self.font_small)\n",
        "            # painter.setPen(RED_COLOR)\n",
        "            # painter.drawText(cx + 15, cy - 15, f\"Smooth X: {cx}\")\n",
        "            # painter.drawText(cx + 15, cy, f\"Smooth Y: {cy}\")\n",
        "            # raw_center = self.target_history[-1][1] if self.target_history else QPointF(0,0) # Get latest raw for comparison\n",
        "            # painter.drawText(cx + 15, cy + 15, f\"Raw X: {int(raw_center.x())}\")\n",
        "            # painter.drawText(cx + 15, cy + 30, f\"Raw Y: {int(raw_center.y())}\")\n",
        "\n",
        "\n",
        "        # --- 9. HUD Text ---\n",
        "        if self.show_hud:\n",
        "            painter.setFont(self.font_medium)\n",
        "            painter.setPen(TEXT_COLOR)\n",
        "            y_offset = 20 # Starting Y position for HUD text\n",
        "\n",
        "            # Display basic info\n",
        "            painter.drawText(10, y_offset, f\"Cap: {self.capture_fps:.1f} FPS\")\n",
        "            y_offset += 15\n",
        "            painter.drawText(10, y_offset, f\"Det: {self.detection_fps:.1f} FPS ({self.detection_proc_time:.1f} ms)\")\n",
        "            y_offset += 15\n",
        "            if self.show_poses: # Only show pose info if enabled\n",
        "                 painter.drawText(10, y_offset, f\"Pose: {self.pose_fps:.1f} FPS ({self.pose_proc_time:.1f} ms)\")\n",
        "                 y_offset += 15\n",
        "            if self.show_flow: # Only show flow info if enabled\n",
        "                 painter.drawText(10, y_offset, f\"Flow: {self.flow_fps:.1f} FPS ({self.flow_proc_time:.1f} ms)\") # Assuming you add flow_proc_time\n",
        "                 y_offset += 15\n",
        "            if self.show_depth: # Only show depth info if enabled\n",
        "                 painter.drawText(10, y_offset, f\"Depth: {self.depth_fps:.1f} FPS ({self.depth_proc_time:.1f} ms)\") # Assuming you add depth_proc_time\n",
        "                 y_offset += 15\n",
        "\n",
        "\n",
        "            # Display system info from hud_texts dictionary\n",
        "            for key, value in self.hud_texts.items():\n",
        "                painter.drawText(10, y_offset, f\"{key}: {value}\")\n",
        "                y_offset += 15\n",
        "\n",
        "            # Display Crosshair Smoothing Info in HUD\n",
        "            if self.show_crosshair and self.smoothed_target_center:\n",
        "                 sx, sy = int(self.smoothed_target_center.x()), int(self.smoothed_target_center.y())\n",
        "                 painter.drawText(10, y_offset, f\"Crosshair (Smooth): {sx}, {sy}\")\n",
        "                 y_offset += 15\n",
        "                 # Display raw if available for comparison\n",
        "                 # raw_center = QPointF(0,0)\n",
        "                 # if self.target_history:\n",
        "                 #      # Find the most recent non-smoothed center if possible (difficult with current EMA)\n",
        "                 #      # Simplification: Show the latest smoothed point again or leave raw out\n",
        "                 #      pass\n",
        "                 # painter.drawText(10, y_offset, f\"Crosshair (Raw): {int(raw_center.x())}, {int(raw_center.y())}\")\n",
        "                 # y_offset += 15\n",
        "\n",
        "\n",
        "        painter.end()\n",
        "\n",
        "\n",
        "# --- Main Application Window ---\n",
        "class MainWindow(QMainWindow):\n",
        "    \"\"\"Main application window with controls.\"\"\"\n",
        "    # Signals to control worker threads\n",
        "    capture_interval_changed = Signal(int)\n",
        "    detection_enabled_changed = Signal(bool)\n",
        "    detection_conf_changed = Signal(float)\n",
        "    detection_nms_changed = Signal(float)\n",
        "    pose_enabled_changed = Signal(bool) # Signal for pose estimation enable/disable\n",
        "    pose_conf_changed = Signal(float)   # Signal for pose confidence threshold\n",
        "    flow_enabled_changed = Signal(bool)\n",
        "    depth_enabled_changed = Signal(bool)\n",
        "\n",
        "    # Signals to update overlay drawing toggles\n",
        "    toggle_boxes_signal = Signal(bool)\n",
        "    toggle_labels_signal = Signal(bool)\n",
        "    toggle_paths_signal = Signal(bool)\n",
        "    toggle_trajectory_signal = Signal(bool)\n",
        "    toggle_crosshair_signal = Signal(bool)\n",
        "    toggle_poses_signal = Signal(bool) # Signal for toggling pose drawing\n",
        "    toggle_flow_signal = Signal(bool)\n",
        "    toggle_depth_signal = Signal(bool)\n",
        "    toggle_hud_signal = Signal(bool)\n",
        "\n",
        "    # Signal to send HUD data\n",
        "    hud_data_signal = Signal(dict)\n",
        "    capture_fps_signal = Signal(float)\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.setWindowTitle(\"Real-Time Detection Overlay\")\n",
        "        self.setGeometry(100, 100, 450, 700) # Main window size\n",
        "\n",
        "        # --- Monitor Selection ---\n",
        "        self.monitors = mss.mss().monitors[1:] # Exclude the 'all monitors' entry\n",
        "        self.selected_monitor_spec = self.monitors[0] # Default to the first monitor\n",
        "\n",
        "        # --- Central Widget and Layout ---\n",
        "        self.central_widget = QWidget()\n",
        "        self.main_layout = QVBoxLayout(self.central_widget)\n",
        "        self.setCentralWidget(self.central_widget)\n",
        "\n",
        "        # --- Control Panel ---\n",
        "        self.control_group = QGroupBox(\"Controls\")\n",
        "        self.control_layout = QGridLayout()\n",
        "        self.control_group.setLayout(self.control_layout)\n",
        "        self.main_layout.addWidget(self.control_group)\n",
        "\n",
        "        # Monitor Selection\n",
        "        self.monitor_combo = QComboBox()\n",
        "        self.monitor_combo.addItems([f\"Monitor {i+1} ({m['width']}x{m['height']})\" for i, m in enumerate(self.monitors)])\n",
        "        self.monitor_combo.currentIndexChanged.connect(self.update_monitor)\n",
        "        self.control_layout.addWidget(QLabel(\"Target Monitor:\"), 0, 0)\n",
        "        self.control_layout.addWidget(self.monitor_combo, 0, 1, 1, 2) # Span 2 columns\n",
        "\n",
        "        # Capture Interval\n",
        "        self.interval_spinbox = QSpinBox()\n",
        "        self.interval_spinbox.setRange(1, 1000) # 1ms to 1s\n",
        "        self.interval_spinbox.setValue(CAPTURE_INTERVAL_MS)\n",
        "        self.interval_spinbox.setSuffix(\" ms\")\n",
        "        self.interval_spinbox.valueChanged.connect(self.capture_interval_changed.emit)\n",
        "        self.control_layout.addWidget(QLabel(\"Capture Interval:\"), 1, 0)\n",
        "        self.control_layout.addWidget(self.interval_spinbox, 1, 1, 1, 2)\n",
        "\n",
        "        # --- Detection Controls ---\n",
        "        self.detection_group = QGroupBox(\"Object Detection (YOLOv8n)\")\n",
        "        self.detection_layout = QGridLayout()\n",
        "        self.detection_group.setLayout(self.detection_layout)\n",
        "        self.main_layout.addWidget(self.detection_group)\n",
        "\n",
        "        self.detection_enabled_check = QCheckBox(\"Enable Detection\")\n",
        "        self.detection_enabled_check.setChecked(True)\n",
        "        self.detection_enabled_check.toggled.connect(self.detection_enabled_changed.emit)\n",
        "        self.detection_layout.addWidget(self.detection_enabled_check, 0, 0, 1, 3) # Span columns\n",
        "\n",
        "        self.conf_spinbox = QDoubleSpinBox()\n",
        "        self.conf_spinbox.setRange(0.01, 1.0)\n",
        "        self.conf_spinbox.setSingleStep(0.05)\n",
        "        self.conf_spinbox.setValue(DEFAULT_CONFIDENCE_THRESHOLD)\n",
        "        self.conf_spinbox.valueChanged.connect(self.detection_conf_changed.emit)\n",
        "        self.detection_layout.addWidget(QLabel(\"Confidence Threshold:\"), 1, 0)\n",
        "        self.detection_layout.addWidget(self.conf_spinbox, 1, 1, 1, 2)\n",
        "\n",
        "        self.nms_spinbox = QDoubleSpinBox()\n",
        "        self.nms_spinbox.setRange(0.01, 1.0)\n",
        "        self.nms_spinbox.setSingleStep(0.05)\n",
        "        self.nms_spinbox.setValue(DEFAULT_NMS_THRESHOLD)\n",
        "        self.nms_spinbox.valueChanged.connect(self.detection_nms_changed.emit)\n",
        "        self.detection_layout.addWidget(QLabel(\"NMS Threshold:\"), 2, 0)\n",
        "        self.detection_layout.addWidget(self.nms_spinbox, 2, 1, 1, 2)\n",
        "\n",
        "        # --- Pose Estimation Controls ---\n",
        "        self.pose_group = QGroupBox(\"Pose Estimation (YOLOv8n-Pose)\")\n",
        "        self.pose_layout = QGridLayout()\n",
        "        self.pose_group.setLayout(self.pose_layout)\n",
        "        self.main_layout.addWidget(self.pose_group)\n",
        "\n",
        "        self.pose_enabled_check = QCheckBox(\"Enable Pose Estimation\")\n",
        "        self.pose_enabled_check.setChecked(False) # Default disabled\n",
        "        self.pose_enabled_check.toggled.connect(self.pose_enabled_changed.emit)\n",
        "        self.pose_enabled_check.toggled.connect(self.toggle_poses_signal.emit) # Also toggle drawing\n",
        "        self.pose_layout.addWidget(self.pose_enabled_check, 0, 0, 1, 3)\n",
        "\n",
        "        self.pose_conf_spinbox = QDoubleSpinBox()\n",
        "        self.pose_conf_spinbox.setRange(0.01, 1.0)\n",
        "        self.pose_conf_spinbox.setSingleStep(0.05)\n",
        "        self.pose_conf_spinbox.setValue(DEFAULT_POSE_CONFIDENCE_THRESHOLD)\n",
        "        self.pose_conf_spinbox.valueChanged.connect(self.pose_conf_changed.emit)\n",
        "        self.pose_layout.addWidget(QLabel(\"Pose Conf Threshold:\"), 1, 0)\n",
        "        self.pose_layout.addWidget(self.pose_conf_spinbox, 1, 1, 1, 2)\n",
        "\n",
        "\n",
        "        # --- Other Feature Controls ---\n",
        "        self.features_group = QGroupBox(\"Other Features\")\n",
        "        self.features_layout = QGridLayout()\n",
        "        self.features_group.setLayout(self.features_layout)\n",
        "        self.main_layout.addWidget(self.features_group)\n",
        "\n",
        "        self.flow_enabled_check = QCheckBox(\"Enable Optical Flow\")\n",
        "        self.flow_enabled_check.setChecked(False)\n",
        "        self.flow_enabled_check.toggled.connect(self.flow_enabled_changed.emit)\n",
        "        self.flow_enabled_check.toggled.connect(self.toggle_flow_signal.emit)\n",
        "        self.features_layout.addWidget(self.flow_enabled_check, 0, 0)\n",
        "\n",
        "        self.depth_enabled_check = QCheckBox(\"Enable Depth Estimation\")\n",
        "        self.depth_enabled_check.setChecked(False)\n",
        "        self.depth_enabled_check.toggled.connect(self.depth_enabled_changed.emit)\n",
        "        self.depth_enabled_check.toggled.connect(self.toggle_depth_signal.emit)\n",
        "        self.features_layout.addWidget(self.depth_enabled_check, 0, 1)\n",
        "\n",
        "\n",
        "        # --- Overlay Visibility Controls ---\n",
        "        self.visibility_group = QGroupBox(\"Overlay Visibility\")\n",
        "        self.visibility_layout = QGridLayout()\n",
        "        self.visibility_group.setLayout(self.visibility_layout)\n",
        "        self.main_layout.addWidget(self.visibility_group)\n",
        "\n",
        "        self.show_boxes_check = QCheckBox(\"Show Boxes\")\n",
        "        self.show_boxes_check.setChecked(True)\n",
        "        self.show_boxes_check.toggled.connect(self.toggle_boxes_signal.emit)\n",
        "        self.visibility_layout.addWidget(self.show_boxes_check, 0, 0)\n",
        "\n",
        "        self.show_labels_check = QCheckBox(\"Show Labels\")\n",
        "        self.show_labels_check.setChecked(True)\n",
        "        self.show_labels_check.toggled.connect(self.toggle_labels_signal.emit)\n",
        "        self.visibility_layout.addWidget(self.show_labels_check, 0, 1)\n",
        "\n",
        "        self.show_paths_check = QCheckBox(\"Show Paths\")\n",
        "        self.show_paths_check.setChecked(True)\n",
        "        self.show_paths_check.toggled.connect(self.toggle_paths_signal.emit)\n",
        "        self.visibility_layout.addWidget(self.show_paths_check, 1, 0)\n",
        "\n",
        "        self.show_trajectory_check = QCheckBox(\"Show Trajectory\")\n",
        "        self.show_trajectory_check.setChecked(True)\n",
        "        self.show_trajectory_check.toggled.connect(self.toggle_trajectory_signal.emit)\n",
        "        self.visibility_layout.addWidget(self.show_trajectory_check, 1, 1)\n",
        "\n",
        "        self.show_crosshair_check = QCheckBox(\"Show Crosshair\")\n",
        "        self.show_crosshair_check.setChecked(True)\n",
        "        self.show_crosshair_check.toggled.connect(self.toggle_crosshair_signal.emit)\n",
        "        self.visibility_layout.addWidget(self.show_crosshair_check, 2, 0)\n",
        "\n",
        "        # Pose visibility is linked to the enable checkbox, but keep separate toggle signal\n",
        "        # self.show_poses_check = QCheckBox(\"Show Poses\") # Redundant if linked to enable\n",
        "        # self.show_poses_check.setChecked(self.pose_enabled_check.isChecked())\n",
        "        # self.show_poses_check.toggled.connect(self.toggle_poses_signal.emit)\n",
        "        # self.visibility_layout.addWidget(self.show_poses_check, 2, 1)\n",
        "\n",
        "        self.show_hud_check = QCheckBox(\"Show HUD\")\n",
        "        self.show_hud_check.setChecked(True)\n",
        "        self.show_hud_check.toggled.connect(self.toggle_hud_signal.emit)\n",
        "        self.visibility_layout.addWidget(self.show_hud_check, 2, 1) # Move HUD toggle here\n",
        "\n",
        "\n",
        "        # --- Log Output ---\n",
        "        self.log_group = QGroupBox(\"Log Output\")\n",
        "        self.log_layout = QVBoxLayout()\n",
        "        self.log_group.setLayout(self.log_layout)\n",
        "        self.log_text_edit = QTextEdit()\n",
        "        self.log_text_edit.setReadOnly(True)\n",
        "        self.log_layout.addWidget(self.log_text_edit)\n",
        "        self.main_layout.addWidget(self.log_group)\n",
        "\n",
        "        # --- Status Bar ---\n",
        "        self.status_bar = self.statusBar()\n",
        "        self.status_bar.showMessage(\"Ready\")\n",
        "\n",
        "        # --- Initialize Overlay ---\n",
        "        monitor_qrect = QRect(\n",
        "            self.selected_monitor_spec['left'],\n",
        "            self.selected_monitor_spec['top'],\n",
        "            self.selected_monitor_spec['width'],\n",
        "            self.selected_monitor_spec['height']\n",
        "        )\n",
        "        self.overlay = OverlayWidget(monitor_qrect)\n",
        "        self.overlay.show()\n",
        "\n",
        "        # --- Setup Logging Handler ---\n",
        "        self.log_handler = QTextEditLogger(self.log_text_edit)\n",
        "        logging.getLogger().addHandler(self.log_handler)\n",
        "        logging.getLogger().setLevel(logging.INFO) # Ensure root logger level is appropriate\n",
        "\n",
        "        # --- System Info Timer ---\n",
        "        self.system_info_timer = QTimer(self)\n",
        "        self.system_info_timer.timeout.connect(self.update_system_info)\n",
        "        self.system_info_timer.start(SYSTEM_INFO_INTERVAL_MS)\n",
        "\n",
        "        # --- Initialize Threads ---\n",
        "        self.capture_thread = None\n",
        "        self.detection_thread = None\n",
        "        self.pose_thread = None\n",
        "        self.flow_thread = None\n",
        "        self.depth_thread = None\n",
        "        self.start_threads()\n",
        "\n",
        "        # --- Connect Signals ---\n",
        "        self.connect_signals()\n",
        "\n",
        "        # --- Initial System Info Update ---\n",
        "        self.update_system_info()\n",
        "\n",
        "\n",
        "    def update_monitor(self, index):\n",
        "        \"\"\"Restarts threads when the monitor selection changes.\"\"\"\n",
        "        if 0 <= index < len(self.monitors):\n",
        "            self.selected_monitor_spec = self.monitors[index]\n",
        "            logger.info(f\"Monitor changed to: {index+1}\")\n",
        "            self.status_bar.showMessage(f\"Monitor changed to {index+1}. Restarting capture...\")\n",
        "            # Stop existing threads\n",
        "            self.stop_threads()\n",
        "            # Update overlay geometry\n",
        "            monitor_qrect = QRect(\n",
        "                self.selected_monitor_spec['left'],\n",
        "                self.selected_monitor_spec['top'],\n",
        "                self.selected_monitor_spec['width'],\n",
        "                self.selected_monitor_spec['height']\n",
        "            )\n",
        "            self.overlay.setGeometry(monitor_qrect)\n",
        "            self.overlay.monitor_rect = monitor_qrect # Update internal rect too\n",
        "            # Restart threads with the new monitor spec\n",
        "            self.start_threads()\n",
        "            # Reconnect signals as threads are new instances\n",
        "            self.connect_signals() # Reconnect is crucial\n",
        "            self.status_bar.showMessage(f\"Capture restarted on monitor {index+1}.\")\n",
        "        else:\n",
        "             logger.error(f\"Invalid monitor index: {index}\")\n",
        "\n",
        "\n",
        "    def start_threads(self):\n",
        "        \"\"\"Initializes and starts all worker threads.\"\"\"\n",
        "        logger.info(\"Starting worker threads...\")\n",
        "        # Screen Capture\n",
        "        self.capture_thread = ScreenCaptureThread(self.selected_monitor_spec)\n",
        "        self.capture_thread.status_update.connect(self.update_status)\n",
        "        self.capture_thread.start()\n",
        "\n",
        "        # Detection\n",
        "        self.detection_thread = DetectionThread(OBJECT_DETECTION_MODEL)\n",
        "        self.detection_thread.status_update.connect(self.update_status)\n",
        "        self.detection_thread.start()\n",
        "\n",
        "        # Pose Estimation\n",
        "        self.pose_thread = PoseEstimationThread(POSE_ESTIMATION_MODEL)\n",
        "        self.pose_thread.status_update.connect(self.update_status)\n",
        "        self.pose_thread.start()\n",
        "\n",
        "        # Optical Flow\n",
        "        self.flow_thread = OpticalFlowThread()\n",
        "        self.flow_thread.status_update.connect(self.update_status)\n",
        "        self.flow_thread.start()\n",
        "\n",
        "        # Depth Estimation\n",
        "        self.depth_thread = DepthEstimationThread() # Uses default MiDaS_small\n",
        "        self.depth_thread.status_update.connect(self.update_status)\n",
        "        self.depth_thread.start()\n",
        "\n",
        "        logger.info(\"Worker threads started.\")\n",
        "\n",
        "\n",
        "    def connect_signals(self):\n",
        "         \"\"\"Connects signals between GUI, threads, and overlay.\"\"\"\n",
        "         logger.info(\"Connecting signals...\")\n",
        "         # --- Capture Thread Connections ---\n",
        "         if self.capture_thread:\n",
        "             self.capture_thread.frame_ready.connect(self.calculate_capture_fps) # Connect to FPS calc first\n",
        "             # Connect frame_ready to worker threads that need the frame\n",
        "             if self.detection_thread:\n",
        "                 self.capture_thread.frame_ready.connect(self.detection_thread.set_frame)\n",
        "             if self.pose_thread:\n",
        "                 self.capture_thread.frame_ready.connect(self.pose_thread.set_frame)\n",
        "             if self.flow_thread:\n",
        "                 self.capture_thread.frame_ready.connect(self.flow_thread.set_frame)\n",
        "             if self.depth_thread:\n",
        "                 self.capture_thread.frame_ready.connect(self.depth_thread.set_frame)\n",
        "             # Connect GUI controls to capture thread slots\n",
        "             self.capture_interval_changed.connect(self.capture_thread.update_capture_interval)\n",
        "\n",
        "\n",
        "         # --- Detection Thread Connections ---\n",
        "         if self.detection_thread:\n",
        "             self.detection_thread.detections_ready.connect(self.overlay.update_detections)\n",
        "             # Connect GUI controls to detection thread slots\n",
        "             self.detection_enabled_changed.connect(self.detection_thread.set_enabled)\n",
        "             self.detection_conf_changed.connect(self.detection_thread.update_confidence_threshold)\n",
        "             self.detection_nms_changed.connect(self.detection_thread.update_nms_threshold)\n",
        "             # Initial state sync\n",
        "             self.detection_thread.set_enabled(self.detection_enabled_check.isChecked())\n",
        "             self.detection_thread.update_confidence_threshold(self.conf_spinbox.value())\n",
        "             self.detection_thread.update_nms_threshold(self.nms_spinbox.value())\n",
        "\n",
        "\n",
        "         # --- Pose Thread Connections ---\n",
        "         if self.pose_thread:\n",
        "             self.pose_thread.poses_ready.connect(self.overlay.update_poses)\n",
        "             # Connect GUI controls to pose thread slots\n",
        "             self.pose_enabled_changed.connect(self.pose_thread.set_enabled)\n",
        "             self.pose_conf_changed.connect(self.pose_thread.update_confidence_threshold)\n",
        "              # Initial state sync\n",
        "             self.pose_thread.set_enabled(self.pose_enabled_check.isChecked())\n",
        "             self.pose_thread.update_confidence_threshold(self.pose_conf_spinbox.value())\n",
        "\n",
        "\n",
        "         # --- Flow Thread Connections ---\n",
        "         if self.flow_thread:\n",
        "             self.flow_thread.flow_ready.connect(self.overlay.update_flow)\n",
        "             # Connect GUI controls to flow thread slots\n",
        "             self.flow_enabled_changed.connect(self.flow_thread.set_enabled)\n",
        "             # Initial state sync\n",
        "             self.flow_thread.set_enabled(self.flow_enabled_check.isChecked())\n",
        "\n",
        "\n",
        "         # --- Depth Thread Connections ---\n",
        "         if self.depth_thread:\n",
        "             self.depth_thread.depth_ready.connect(self.overlay.update_depth)\n",
        "             # Connect GUI controls to depth thread slots\n",
        "             self.depth_enabled_changed.connect(self.depth_thread.set_enabled)\n",
        "             # Initial state sync\n",
        "             self.depth_thread.set_enabled(self.depth_enabled_check.isChecked())\n",
        "\n",
        "\n",
        "         # --- Overlay Visibility Connections ---\n",
        "         self.toggle_boxes_signal.connect(self.overlay.toggle_boxes)\n",
        "         self.toggle_labels_signal.connect(self.overlay.toggle_labels)\n",
        "         self.toggle_paths_signal.connect(self.overlay.toggle_paths)\n",
        "         self.toggle_trajectory_signal.connect(self.overlay.toggle_trajectory)\n",
        "         self.toggle_crosshair_signal.connect(self.overlay.toggle_crosshair)\n",
        "         self.toggle_poses_signal.connect(self.overlay.toggle_poses) # Connect pose visibility\n",
        "         self.toggle_flow_signal.connect(self.overlay.toggle_flow)\n",
        "         self.toggle_depth_signal.connect(self.overlay.toggle_depth)\n",
        "         self.toggle_hud_signal.connect(self.overlay.toggle_hud)\n",
        "         # Initial state sync for visibility toggles\n",
        "         self.overlay.toggle_boxes(self.show_boxes_check.isChecked())\n",
        "         self.overlay.toggle_labels(self.show_labels_check.isChecked())\n",
        "         self.overlay.toggle_paths(self.show_paths_check.isChecked())\n",
        "         self.overlay.toggle_trajectory(self.show_trajectory_check.isChecked())\n",
        "         self.overlay.toggle_crosshair(self.show_crosshair_check.isChecked())\n",
        "         self.overlay.toggle_poses(self.pose_enabled_check.isChecked()) # Link initial pose visibility to enable state\n",
        "         self.overlay.toggle_flow(self.flow_enabled_check.isChecked()) # Link initial flow visibility to enable state\n",
        "         self.overlay.toggle_depth(self.depth_enabled_check.isChecked()) # Link initial depth visibility to enable state\n",
        "         self.overlay.toggle_hud(self.show_hud_check.isChecked())\n",
        "\n",
        "\n",
        "         # --- HUD Data Connection ---\n",
        "         self.hud_data_signal.connect(self.overlay.update_hud)\n",
        "         self.capture_fps_signal.connect(self.overlay.update_capture_fps)\n",
        "\n",
        "\n",
        "         logger.info(\"Signals connected.\")\n",
        "\n",
        "\n",
        "    def stop_threads(self):\n",
        "        \"\"\"Stops all worker threads gracefully.\"\"\"\n",
        "        logger.info(\"Stopping worker threads...\")\n",
        "        if self.capture_thread:\n",
        "            self.capture_thread.stop()\n",
        "            self.capture_thread.wait() # Wait for thread to finish\n",
        "            self.capture_thread = None\n",
        "        if self.detection_thread:\n",
        "            self.detection_thread.stop()\n",
        "            self.detection_thread.wait()\n",
        "            self.detection_thread = None\n",
        "        if self.pose_thread:\n",
        "            self.pose_thread.stop()\n",
        "            self.pose_thread.wait()\n",
        "            self.pose_thread = None\n",
        "        if self.flow_thread:\n",
        "            self.flow_thread.stop()\n",
        "            self.flow_thread.wait()\n",
        "            self.flow_thread = None\n",
        "        if self.depth_thread:\n",
        "            self.depth_thread.stop()\n",
        "            self.depth_thread.wait()\n",
        "            self.depth_thread = None\n",
        "        logger.info(\"Worker threads stopped.\")\n",
        "\n",
        "\n",
        "    @Slot(str)\n",
        "    def update_status(self, message):\n",
        "        \"\"\"Updates the status bar.\"\"\"\n",
        "        self.status_bar.showMessage(message, 5000) # Show for 5 seconds\n",
        "\n",
        "\n",
        "    def update_system_info(self):\n",
        "        \"\"\"Fetches system info and sends it to the HUD.\"\"\"\n",
        "        cpu_usage = psutil.cpu_percent()\n",
        "        memory_info = psutil.virtual_memory()\n",
        "        mem_usage = f\"{memory_info.percent}% ({memory_info.used / (1024**3):.1f}/{memory_info.total / (1024**3):.1f} GB)\"\n",
        "        gpu_name, gpu_mem_usage = get_gpu_info()\n",
        "\n",
        "        hud_data = {\n",
        "            \"CPU\": f\"{cpu_usage:.1f}%\",\n",
        "            \"RAM\": mem_usage,\n",
        "            \"GPU\": gpu_name,\n",
        "            \"VRAM\": gpu_mem_usage,\n",
        "            \"Time\": datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
        "        }\n",
        "        self.hud_data_signal.emit(hud_data)\n",
        "\n",
        "\n",
        "    # --- FPS Calculation Slot ---\n",
        "    @Slot(np.ndarray, float)\n",
        "    def calculate_capture_fps(self, frame, timestamp):\n",
        "        \"\"\"Calculates capture FPS based on frame arrival times.\"\"\"\n",
        "        now = time.perf_counter()\n",
        "        time_diff = now - self.overlay.last_capture_time # Use overlay's time tracker\n",
        "        if time_diff > 0:\n",
        "            fps = 1.0 / time_diff\n",
        "            self.capture_fps_signal.emit(fps) # Emit the calculated FPS\n",
        "        self.overlay.last_capture_time = now # Update the last time\n",
        "\n",
        "\n",
        "    def closeEvent(self, event):\n",
        "        \"\"\"Ensures threads and overlay are cleaned up on exit.\"\"\"\n",
        "        logger.info(\"Close event triggered. Cleaning up...\")\n",
        "        self.stop_threads()\n",
        "        if self.overlay:\n",
        "            self.overlay.close() # Close the overlay window\n",
        "        logger.info(\"Cleanup complete. Exiting.\")\n",
        "        event.accept()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure necessary model files are present or downloaded by ultralytics\n",
        "    # You might need to run `yolo predict model=yolov8n.pt source=0` or similar once\n",
        "    # for ultralytics to download the models if they aren't cached.\n",
        "    try:\n",
        "        # Check if models exist, attempt download if not (basic check)\n",
        "        if not os.path.exists(OBJECT_DETECTION_MODEL):\n",
        "             logger.info(f\"Attempting to download {OBJECT_DETECTION_MODEL}...\")\n",
        "             _ = YOLO(OBJECT_DETECTION_MODEL) # Instantiating should trigger download\n",
        "        if not os.path.exists(POSE_ESTIMATION_MODEL):\n",
        "             logger.info(f\"Attempting to download {POSE_ESTIMATION_MODEL}...\")\n",
        "             _ = YOLO(POSE_ESTIMATION_MODEL)\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not pre-download/verify models: {e}. YOLO will attempt download on first use.\")\n",
        "\n",
        "\n",
        "    # Set high DPI scaling for better rendering on some systems\n",
        "    QApplication.setAttribute(Qt.AA_EnableHighDpiScaling, True)\n",
        "    QApplication.setAttribute(Qt.AA_UseHighDpiPixmaps, True)\n",
        "\n",
        "    app = QApplication(sys.argv)\n",
        "    main_window = MainWindow()\n",
        "    main_window.show()\n",
        "    sys.exit(app.exec())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "4O3G3mTGr0YM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}