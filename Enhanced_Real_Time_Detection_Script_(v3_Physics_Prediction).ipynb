{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mss\n",
        "import psutil\n",
        "import pyautogui # Keep for potential future use, but not used in current drawing logic\n",
        "from PySide6 import QtCore, QtGui, QtWidgets\n",
        "from PySide6.QtCore import Qt, QTimer, QPoint, QPointF, QThread, Signal, Slot, QMutex, QMutexLocker, QObject, QRect\n",
        "from PySide6.QtGui import QColor, QFont, QPainter, QPen, QFontDatabase, QImage, QPixmap, QPolygonF\n",
        "from PySide6.QtWidgets import (QApplication, QWidget, QMainWindow, QVBoxLayout, QGridLayout,\n",
        "                             QComboBox, QSpinBox, QDoubleSpinBox, QPushButton, QLabel,\n",
        "                             QTextEdit, QCheckBox, QGroupBox)\n",
        "import logging\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import os\n",
        "import collections # Added for deque\n",
        "\n",
        "# --- Configurations ---\n",
        "UPDATE_INTERVAL_MS = 500  # Interval for updating HUD text elements\n",
        "SCANLINE_SPEED_MS = 15    # Speed of the scanline effect\n",
        "CAPTURE_INTERVAL_MS = 10  # Target capture interval (100 FPS)\n",
        "SYSTEM_INFO_INTERVAL_MS = 1000 # Interval for updating system info\n",
        "DEFAULT_CONFIDENCE_THRESHOLD = 0.4 # Default confidence threshold for detection\n",
        "DEFAULT_NMS_THRESHOLD = 0.3        # Default NMS threshold\n",
        "DEFAULT_POSE_CONFIDENCE_THRESHOLD = 0.5 # Default confidence for pose keypoints\n",
        "FONT_NAME = \"Press Start 2P\"\n",
        "FALLBACK_FONT = \"Monospace\"\n",
        "FONT_SIZE_SMALL = 10\n",
        "FONT_SIZE_MEDIUM = 12\n",
        "RED_COLOR = QColor(255, 0, 0)\n",
        "GREEN_COLOR = QColor(0, 255, 0)\n",
        "BLUE_COLOR = QColor(0, 0, 255)\n",
        "YELLOW_COLOR = QColor(255, 255, 0)\n",
        "CYAN_COLOR = QColor(0, 255, 255)\n",
        "MAGENTA_COLOR = QColor(255, 0, 255)\n",
        "ORANGE_COLOR = QColor(255, 165, 0) # Added orange\n",
        "TEXT_COLOR = QColor(255, 0, 0)\n",
        "\n",
        "# --- Smoothing and Prediction Parameters ---\n",
        "TARGET_CENTER_SMOOTHING_FACTOR = 0.3 # Alpha for EMA (lower = smoother, more lag) - For position\n",
        "PATH_HISTORY_LENGTH = 30             # Number of past points to store for path tracking\n",
        "# --- Velocity Smoothing ---\n",
        "VELOCITY_SMOOTHING_FACTOR = 0.4      # Alpha for EMA for velocity vector (renamed from TRAJECTORY_SMOOTHING_FACTOR)\n",
        "VELOCITY_HISTORY_LENGTH = 15         # Number of recent velocity samples to store for acceleration calculation\n",
        "# --- Acceleration Smoothing ---\n",
        "ACCEL_SMOOTHING_FACTOR = 0.5         # Alpha for EMA for acceleration vector\n",
        "# --- Trajectory Prediction ---\n",
        "TRAJECTORY_PREDICTION_POINTS = 5     # Number of recent points to use for initial velocity calculation (kept for reference, but direct velocity smoothing is primary)\n",
        "TRAJECTORY_PREDICTION_DURATION = 0.5 # Seconds into the future to predict (slightly increased)\n",
        "PREDICTION_TIME_STEP = 1.0 / 30.0    # Time step for physics prediction loop (e.g., 30 steps per second)\n",
        "\n",
        "\n",
        "# --- Model Names ---\n",
        "OBJECT_DETECTION_MODEL = 'yolov8n.pt'\n",
        "POSE_ESTIMATION_MODEL = 'yolov8n-pose.pt'\n",
        "\n",
        "# --- Pose Estimation Keypoint Connections (COCO format) ---\n",
        "POSE_CONNECTIONS = [\n",
        "    (0, 1), (0, 2), (1, 3), (2, 4),  # Head\n",
        "    (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # Torso/Arms\n",
        "    (11, 12), (5, 11), (6, 12), # Shoulders to Hips\n",
        "    (11, 13), (13, 15), (12, 14), (14, 16)  # Legs\n",
        "]\n",
        "POSE_COLORS = [QColor(255, 0, 0), QColor(0, 255, 0), QColor(0, 0, 255),\n",
        "               QColor(255, 255, 0), QColor(0, 255, 255), QColor(255, 0, 255)]\n",
        "\n",
        "\n",
        "# --- Logging Setup ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(threadName)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def random_hex(length):\n",
        "    \"\"\"Generate a random hexadecimal string of specified length.\"\"\"\n",
        "    return ''.join(random.choice('ABCDEF0123456789') for _ in range(length))\n",
        "\n",
        "def get_gpu_info():\n",
        "    \"\"\"Fetches GPU name and memory usage if CUDA is available.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        try:\n",
        "            # Use recommended mem_get_info if available\n",
        "            total_mem, free_mem = torch.cuda.mem_get_info(0)\n",
        "            used_mem = total_mem - free_mem\n",
        "            mem_usage = f\"{(used_mem / (1024**3)):.1f}/{(total_mem / (1024**3)):.1f} GB\"\n",
        "        except AttributeError:\n",
        "            # Fallback for older PyTorch versions\n",
        "            total_mem = torch.cuda.get_device_properties(0).total_memory\n",
        "            used_mem = torch.cuda.memory_allocated(0)\n",
        "            mem_usage = f\"{(used_mem / (1024**3)):.1f}/{(total_mem / (1024**3)):.1f} GB (Allocated)\"\n",
        "        except Exception as e:\n",
        "             logger.error(f\"Error getting GPU memory info: {e}\", exc_info=True)\n",
        "             mem_usage = \"N/A (Error reading memory)\"\n",
        "        return gpu_name, mem_usage\n",
        "    return \"N/A (CUDA not available)\", \"N/A\"\n",
        "\n",
        "# --- Custom Logging Handler ---\n",
        "class QTextEditLogger(logging.Handler):\n",
        "    \"\"\"Sends log records to a QTextEdit widget in a thread-safe manner.\"\"\"\n",
        "    def __init__(self, text_edit):\n",
        "        super().__init__()\n",
        "        self.text_edit = text_edit\n",
        "        self.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
        "\n",
        "    def emit(self, record):\n",
        "        msg = self.format(record)\n",
        "        # Use invokeMethod for thread-safe GUI updates\n",
        "        QtCore.QMetaObject.invokeMethod(\n",
        "            self.text_edit,\n",
        "            \"append\",\n",
        "            QtCore.Qt.QueuedConnection, # Ensure update happens in GUI thread\n",
        "            QtCore.Q_ARG(str, msg)\n",
        "        )\n",
        "\n",
        "# --- Screen Capture Thread ---\n",
        "class ScreenCaptureThread(QThread):\n",
        "    \"\"\"Captures the screen at a specified interval.\"\"\"\n",
        "    frame_ready = Signal(np.ndarray, float) # Emit frame (BGR) and capture timestamp\n",
        "    status_update = Signal(str)\n",
        "\n",
        "    def __init__(self, monitor_spec):\n",
        "        super().__init__()\n",
        "        self.monitor_spec = monitor_spec\n",
        "        self.running = False\n",
        "        self.sct = None\n",
        "        self._capture_interval_ms = CAPTURE_INTERVAL_MS\n",
        "        self._lock = QMutex() # Mutex for thread-safe access to interval\n",
        "\n",
        "    def run(self):\n",
        "        self.running = True\n",
        "        try:\n",
        "            self.sct = mss.mss() # Initialize screen capture object\n",
        "            logger.info(f\"Screen capture started for monitor: {self.monitor_spec}\")\n",
        "            # last_capture_time = time.perf_counter() # Use high-resolution timer\n",
        "\n",
        "            while self.running:\n",
        "                capture_start_time = time.perf_counter()\n",
        "                try:\n",
        "                    sct_img = self.sct.grab(self.monitor_spec) # Grab the screen region\n",
        "                    frame = np.array(sct_img) # Convert to numpy array\n",
        "\n",
        "                    # Ensure frame is BGR format (OpenCV standard)\n",
        "                    if frame.shape[2] == 4: # BGRA\n",
        "                        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_BGRA2BGR)\n",
        "                    elif frame.shape[2] == 3: # BGR (or RGB, assume BGR)\n",
        "                        frame_bgr = frame\n",
        "                    else:\n",
        "                        logger.warning(f\"Unexpected frame channel count: {frame.shape[2]}\")\n",
        "                        continue # Skip this frame\n",
        "\n",
        "                    current_time = time.time() # Use system time for timestamping frame data\n",
        "                    self.frame_ready.emit(frame_bgr, current_time) # Emit the captured frame and timestamp\n",
        "\n",
        "                except mss.ScreenShotError as e:\n",
        "                    self.status_update.emit(f\"Screen capture error: {e}\")\n",
        "                    logger.error(f\"Screen capture error: {e}\")\n",
        "                    time.sleep(1) # Wait before retrying on screen capture error\n",
        "                except Exception as e:\n",
        "                    self.status_update.emit(f\"Unexpected screen capture error: {e}\")\n",
        "                    logger.error(f\"Unexpected screen capture error: {e}\", exc_info=True)\n",
        "                    time.sleep(1) # Wait on other errors\n",
        "\n",
        "                # Calculate sleep time to maintain target capture interval\n",
        "                elapsed = time.perf_counter() - capture_start_time\n",
        "                with QMutexLocker(self._lock): # Lock access to interval variable\n",
        "                    interval_sec = self._capture_interval_ms / 1000.0\n",
        "                sleep_time = max(0, interval_sec - elapsed)\n",
        "                if sleep_time > 0:\n",
        "                    time.sleep(sleep_time) # Sleep for the remaining interval time\n",
        "\n",
        "        finally:\n",
        "            if self.sct:\n",
        "                self.sct.close() # Clean up screen capture object\n",
        "            logger.info(\"Screen capture stopped.\")\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Signals the thread to stop running.\"\"\"\n",
        "        self.running = False\n",
        "\n",
        "    @Slot(int)\n",
        "    def update_capture_interval(self, interval):\n",
        "        \"\"\"Updates the screen capture interval (thread-safe).\"\"\"\n",
        "        with QMutexLocker(self._lock): # Ensure thread-safe update\n",
        "            if interval > 0:\n",
        "                logger.info(f\"Updating capture interval to {interval} ms\")\n",
        "                self._capture_interval_ms = interval\n",
        "            else:\n",
        "                 logger.warning(f\"Ignoring invalid capture interval: {interval} ms\")\n",
        "\n",
        "# --- Base Worker Thread for YOLO Models ---\n",
        "class BaseYoloWorker(QThread):\n",
        "    \"\"\"Base class for YOLO detection and pose estimation threads.\"\"\"\n",
        "    status_update = Signal(str) # Signal for status messages\n",
        "\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.model = None # YOLO model instance\n",
        "        self.input_frame = None # Latest frame received\n",
        "        self.frame_timestamp = 0.0 # Timestamp of the latest frame\n",
        "        self.running = False # Flag to control the thread loop\n",
        "        self._enabled = True # Flag to enable/disable processing\n",
        "        self._confidence_threshold = DEFAULT_CONFIDENCE_THRESHOLD\n",
        "        self._nms_threshold = DEFAULT_NMS_THRESHOLD # Used by object detection\n",
        "        self._frame_lock = QMutex() # Mutex for thread-safe frame access\n",
        "        self.device = None # PyTorch device ('cuda' or 'cpu')\n",
        "        self.processing_time_ms = 0.0 # Time taken for the last inference\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Loads the YOLO model onto the appropriate device.\"\"\"\n",
        "        self.status_update.emit(f\"Loading model {self.model_name}...\")\n",
        "        logger.info(f\"Attempting to load YOLO model: {self.model_name}\")\n",
        "        try:\n",
        "            self.model = YOLO(self.model_name) # Load the model using ultralytics library\n",
        "            # Determine device (GPU if available, else CPU)\n",
        "            if torch.cuda.is_available():\n",
        "                self.device = torch.device('cuda')\n",
        "                self.model.to(self.device) # Move model to GPU\n",
        "                device_name = torch.cuda.get_device_name(0)\n",
        "                logger.info(f\"Model '{self.model_name}' loaded on GPU: {device_name}.\")\n",
        "                self.status_update.emit(f\"Model '{self.model_name}' loaded on GPU.\")\n",
        "            else:\n",
        "                self.device = torch.device('cpu')\n",
        "                self.model.to(self.device) # Move model to CPU\n",
        "                logger.info(f\"Model '{self.model_name}' loaded on CPU.\")\n",
        "                self.status_update.emit(f\"Model '{self.model_name}' loaded on CPU.\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Failed to load model '{self.model_name}': {e}\"\n",
        "            logger.error(error_msg, exc_info=True)\n",
        "            self.status_update.emit(error_msg)\n",
        "            self.model = None\n",
        "            self.device = None\n",
        "            return False\n",
        "\n",
        "    @Slot(np.ndarray, float)\n",
        "    def set_frame(self, frame, timestamp):\n",
        "        \"\"\"Receives a new frame for processing (thread-safe).\"\"\"\n",
        "        with QMutexLocker(self._frame_lock):\n",
        "            # Keep only the latest frame and its timestamp\n",
        "            self.input_frame = frame.copy() # Copy frame to avoid issues if source changes\n",
        "            self.frame_timestamp = timestamp\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Main processing loop for the thread.\"\"\"\n",
        "        if not self.load_model(): # Attempt to load the model first\n",
        "            self.running = False\n",
        "            return # Exit if model loading fails\n",
        "\n",
        "        self.running = True\n",
        "        logger.info(f\"{self.__class__.__name__} thread started.\")\n",
        "        while self.running:\n",
        "            if self._enabled: # Only process if enabled\n",
        "                frame_to_process = None\n",
        "                timestamp_to_process = 0.0\n",
        "                # Safely get the latest frame\n",
        "                with QMutexLocker(self._frame_lock):\n",
        "                    if self.input_frame is not None:\n",
        "                        frame_to_process = self.input_frame\n",
        "                        timestamp_to_process = self.frame_timestamp\n",
        "                        self.input_frame = None # Consume the frame so it's not processed again\n",
        "\n",
        "                # Process the frame if one was available\n",
        "                if frame_to_process is not None and self.model is not None:\n",
        "                    start_time = time.perf_counter()\n",
        "                    try:\n",
        "                        # Perform inference using the loaded model\n",
        "                        results = self.model(\n",
        "                            frame_to_process,\n",
        "                            conf=self._confidence_threshold, # Apply confidence threshold\n",
        "                            iou=self._nms_threshold, # Apply NMS threshold\n",
        "                            verbose=False, # Suppress ultralytics console output\n",
        "                            device=self.device # Specify the device for inference\n",
        "                        )\n",
        "                        # Process the results (implemented in subclasses)\n",
        "                        self.process_results(results, timestamp_to_process)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"{self.__class__.__name__} error: {e}\", exc_info=True)\n",
        "                        self.status_update.emit(f\"{self.__class__.__name__} error: {e}\")\n",
        "                    finally:\n",
        "                         # Calculate processing time for this frame\n",
        "                        end_time = time.perf_counter()\n",
        "                        self.processing_time_ms = (end_time - start_time) * 1000\n",
        "\n",
        "                else:\n",
        "                    # No frame available, sleep briefly to avoid busy-waiting\n",
        "                    self.msleep(5) # Sleep for 5 milliseconds\n",
        "            else:\n",
        "                # Thread is disabled, sleep longer\n",
        "                self.msleep(50)\n",
        "\n",
        "        # Cleanup when the thread loop exits\n",
        "        logger.info(f\"{self.__class__.__name__} thread stopped.\")\n",
        "        self.model = None # Release model object\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache() # Clear GPU cache if CUDA was used\n",
        "\n",
        "    def process_results(self, results, timestamp):\n",
        "        \"\"\"Placeholder: Subclasses must implement this to process model output.\"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement process_results\")\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Signals the thread to stop running.\"\"\"\n",
        "        self.running = False\n",
        "\n",
        "    @Slot(bool)\n",
        "    def set_enabled(self, enabled):\n",
        "        \"\"\"Enables or disables processing in the thread.\"\"\"\n",
        "        self._enabled = enabled\n",
        "        logger.info(f\"{self.__class__.__name__} {'enabled' if enabled else 'disabled'}\")\n",
        "        if not enabled:\n",
        "             # Clear potential pending frame when disabled to prevent processing old data\n",
        "             with QMutexLocker(self._frame_lock):\n",
        "                 self.input_frame = None\n",
        "\n",
        "    @Slot(float)\n",
        "    def update_confidence_threshold(self, threshold):\n",
        "        \"\"\"Updates the confidence threshold used for inference.\"\"\"\n",
        "        self._confidence_threshold = threshold\n",
        "        logger.info(f\"{self.__class__.__name__} confidence threshold updated to {threshold:.2f}\")\n",
        "\n",
        "    @Slot(float)\n",
        "    def update_nms_threshold(self, threshold):\n",
        "        \"\"\"Updates the NMS threshold (primarily for object detection).\"\"\"\n",
        "        self._nms_threshold = threshold\n",
        "        logger.info(f\"{self.__class__.__name__} NMS threshold updated to {threshold:.2f}\")\n",
        "\n",
        "    def get_processing_time(self):\n",
        "        \"\"\"Returns the processing time of the last inference.\"\"\"\n",
        "        return self.processing_time_ms\n",
        "\n",
        "# --- Detection Thread ---\n",
        "class DetectionThread(BaseYoloWorker):\n",
        "    \"\"\"Performs object detection using a YOLO model.\"\"\"\n",
        "    # Emits: list of (label, confidence, box_tuple), timestamp, processing_time_ms\n",
        "    detections_ready = Signal(list, float, float)\n",
        "\n",
        "    def __init__(self, model_name=OBJECT_DETECTION_MODEL):\n",
        "        super().__init__(model_name)\n",
        "        # Override base defaults if needed (already set in base class)\n",
        "        # self._confidence_threshold = DEFAULT_CONFIDENCE_THRESHOLD\n",
        "        # self._nms_threshold = DEFAULT_NMS_THRESHOLD\n",
        "\n",
        "    def process_results(self, results, timestamp):\n",
        "        \"\"\"Processes YOLO object detection results and emits them.\"\"\"\n",
        "        detections = []\n",
        "        if results and results[0]: # Check if results are valid\n",
        "            boxes = results[0].boxes # Access the detected boxes\n",
        "            for box in boxes:\n",
        "                # Extract box coordinates, confidence, and class ID\n",
        "                # Move results to CPU before converting to Python types for signal emission\n",
        "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                confidence = box.conf[0].cpu().numpy()\n",
        "                class_id = int(box.cls[0].cpu().numpy())\n",
        "                # Get the class label name from the model's names dictionary\n",
        "                label = self.model.names.get(class_id, f\"ID:{class_id}\")\n",
        "                # Append detection info as a tuple\n",
        "                detections.append((label, float(confidence), (int(x1), int(y1), int(x2), int(y2))))\n",
        "\n",
        "        # Emit the processed detections, timestamp, and processing time\n",
        "        self.detections_ready.emit(detections, timestamp, self.get_processing_time())\n",
        "\n",
        "# --- Pose Estimation Thread ---\n",
        "class PoseEstimationThread(BaseYoloWorker):\n",
        "    \"\"\"Performs pose estimation using a YOLO pose model.\"\"\"\n",
        "    # Emits: list of (keypoints_array, box_tuple, confidence), timestamp, processing_time_ms\n",
        "    # keypoints_array is N x 3 (x, y, confidence) for N keypoints\n",
        "    poses_ready = Signal(list, float, float)\n",
        "\n",
        "    def __init__(self, model_name=POSE_ESTIMATION_MODEL):\n",
        "        super().__init__(model_name)\n",
        "        # Use pose-specific confidence threshold\n",
        "        self._confidence_threshold = DEFAULT_POSE_CONFIDENCE_THRESHOLD\n",
        "        # NMS threshold might be used differently or less relevant for pose\n",
        "        self._nms_threshold = DEFAULT_NMS_THRESHOLD\n",
        "\n",
        "    def process_results(self, results, timestamp):\n",
        "        \"\"\"Processes YOLO pose estimation results and emits them.\"\"\"\n",
        "        poses = []\n",
        "        if results and results[0]: # Check if results are valid\n",
        "            keypoints_list = results[0].keypoints # Access keypoints directly from results\n",
        "            boxes = results[0].boxes # Get associated bounding boxes if needed\n",
        "\n",
        "            # Iterate through each detected pose instance\n",
        "            for i, kpts in enumerate(keypoints_list):\n",
        "                # kpts object contains keypoint data (xy coordinates, confidence)\n",
        "                # Combine them into an N x 3 array (x, y, conf) on the CPU\n",
        "                kpts_data_cpu = kpts.data[0].cpu().numpy() # Get tensor data, move to CPU\n",
        "\n",
        "                # Extract bounding box and overall confidence for this pose instance\n",
        "                box_data = boxes[i]\n",
        "                x1, y1, x2, y2 = box_data.xyxy[0].cpu().numpy()\n",
        "                pose_confidence = box_data.conf[0].cpu().numpy() # Overall confidence for the detected person/pose\n",
        "\n",
        "                # Optional: Filter individual keypoints by confidence before emitting\n",
        "                # e.g., kpts_data_cpu[kpts_data_cpu[:, 2] < KEYPOINT_VISIBILITY_THRESHOLD] = [0, 0, 0]\n",
        "\n",
        "                # Append pose info: keypoints array, bounding box, overall confidence\n",
        "                poses.append((kpts_data_cpu, (int(x1), int(y1), int(x2), int(y2)), float(pose_confidence)))\n",
        "\n",
        "        # Emit the processed poses, timestamp, and processing time\n",
        "        self.poses_ready.emit(poses, timestamp, self.get_processing_time())\n",
        "\n",
        "\n",
        "# --- Optical Flow Thread ---\n",
        "class OpticalFlowThread(QThread):\n",
        "    \"\"\"Calculates sparse optical flow (GPU accelerated if OpenCV CUDA is available).\"\"\"\n",
        "    flow_ready = Signal(list) # List of (start_point_tuple, end_point_tuple)\n",
        "    status_update = Signal(str)\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.running = False\n",
        "        self._enabled = False # Disabled by default\n",
        "        self._frame_lock = QMutex()\n",
        "        self.current_frame = None\n",
        "        self.use_gpu = False # Flag indicating if GPU is used\n",
        "        # GPU components\n",
        "        self.gpu_detector = None\n",
        "        self.gpu_lk_flow = None\n",
        "        self.prev_gray_gpu = None\n",
        "        self.prev_points_gpu = None\n",
        "        # CPU components and parameters\n",
        "        self.cpu_lk_params = dict(winSize=(21, 21), maxLevel=3,\n",
        "                                  criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "        self.cpu_feature_params = dict(maxCorners=100, qualityLevel=0.2, minDistance=7, blockSize=7)\n",
        "        self.prev_gray_cpu = None\n",
        "        self.prev_points_cpu = None\n",
        "        self.processing_time_ms = 0.0\n",
        "\n",
        "        # --- Check for OpenCV CUDA support ---\n",
        "        try:\n",
        "            if cv2.cuda.getCudaEnabledDeviceCount() > 0:\n",
        "                logger.info(\"OpenCV CUDA detected. Attempting to use GPU for Optical Flow.\")\n",
        "                # Ensure parameters match CPU version if desired, converting types as needed\n",
        "                gpu_feature_params = self.cpu_feature_params.copy()\n",
        "                gpu_feature_params['qualityLevel'] = float(gpu_feature_params['qualityLevel'])\n",
        "                gpu_feature_params['minDistance'] = float(gpu_feature_params['minDistance'])\n",
        "\n",
        "                # Create GPU detector and flow calculator\n",
        "                self.gpu_detector = cv2.cuda.createGoodFeaturesToTrackDetector(cv2.CV_8UC1, **gpu_feature_params)\n",
        "                self.gpu_lk_flow = cv2.cuda.SparsePyrLKOpticalFlow_create(\n",
        "                    winSize=self.cpu_lk_params['winSize'],\n",
        "                    maxLevel=self.cpu_lk_params['maxLevel'],\n",
        "                    iters=self.cpu_lk_params['criteria'][2] # Number of iterations from criteria\n",
        "                )\n",
        "                self.use_gpu = True\n",
        "                logger.info(\"Successfully initialized GPU Optical Flow components.\")\n",
        "                self.status_update.emit(\"Optical Flow: Using GPU\")\n",
        "            else:\n",
        "                logger.info(\"No OpenCV CUDA devices found. Using CPU for Optical Flow.\")\n",
        "                self.status_update.emit(\"Optical Flow: Using CPU\")\n",
        "        except AttributeError:\n",
        "            logger.warning(\"OpenCV CUDA module not found or unavailable in this build. Using CPU for Optical Flow.\")\n",
        "            self.status_update.emit(\"Optical Flow: Using CPU (CUDA module unavailable)\")\n",
        "            self.use_gpu = False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing OpenCV CUDA components: {e}. Falling back to CPU.\", exc_info=True)\n",
        "            self.use_gpu = False\n",
        "            self.status_update.emit(\"Optical Flow: Error initializing GPU, using CPU\")\n",
        "\n",
        "\n",
        "    @Slot(np.ndarray, float) # Accept timestamp, though not directly used in flow calc\n",
        "    def set_frame(self, frame, timestamp):\n",
        "        \"\"\"Receives a new frame for processing (thread-safe).\"\"\"\n",
        "        with QMutexLocker(self._frame_lock):\n",
        "            self.current_frame = frame.copy()\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Main processing loop for optical flow calculation.\"\"\"\n",
        "        self.running = True\n",
        "        logger.info(f\"Optical Flow thread started (GPU Enabled: {self.use_gpu}).\")\n",
        "        while self.running:\n",
        "            if self._enabled: # Only process if enabled\n",
        "                frame = None\n",
        "                # Safely get the latest frame\n",
        "                with QMutexLocker(self._frame_lock):\n",
        "                    if self.current_frame is not None:\n",
        "                        frame = self.current_frame\n",
        "                        self.current_frame = None # Consume frame\n",
        "\n",
        "                if frame is not None:\n",
        "                    start_time = time.perf_counter()\n",
        "                    try:\n",
        "                        # Choose GPU or CPU path\n",
        "                        if self.use_gpu:\n",
        "                            self.run_gpu(frame)\n",
        "                        else:\n",
        "                            self.run_cpu(frame)\n",
        "                    except cv2.error as e:\n",
        "                         # Handle OpenCV specific errors\n",
        "                         logger.error(f\"OpenCV error in Optical Flow: {e}\", exc_info=True)\n",
        "                         self.status_update.emit(f\"Optical Flow Error: {e}\")\n",
        "                         self.reset_state() # Reset state on error\n",
        "                         self.msleep(100) # Wait briefly after error\n",
        "                    except Exception as e:\n",
        "                         # Handle other unexpected errors\n",
        "                         logger.error(f\"Unexpected error in Optical Flow: {e}\", exc_info=True)\n",
        "                         self.status_update.emit(f\"Optical Flow Error: {e}\")\n",
        "                         self.reset_state()\n",
        "                         self.msleep(100)\n",
        "                    finally:\n",
        "                        end_time = time.perf_counter()\n",
        "                        self.processing_time_ms = (end_time - start_time) * 1000\n",
        "                else:\n",
        "                     self.msleep(5) # No frame, sleep briefly\n",
        "            else:\n",
        "                # Disabled: Reset state if it wasn't already reset\n",
        "                if self.prev_gray_cpu is not None or self.prev_gray_gpu is not None:\n",
        "                     self.reset_state()\n",
        "                self.msleep(50) # Sleep longer when disabled\n",
        "\n",
        "        logger.info(\"Optical Flow thread stopped.\")\n",
        "        self.reset_state() # Ensure state is clean on exit\n",
        "\n",
        "    def run_gpu(self, frame):\n",
        "        \"\"\"Calculates optical flow using GPU.\"\"\"\n",
        "        frame_gpu = cv2.cuda_GpuMat() # Create GPU matrix\n",
        "        frame_gpu.upload(frame) # Upload frame data to GPU\n",
        "        gray_gpu = cv2.cuda.cvtColor(frame_gpu, cv2.COLOR_BGR2GRAY) # Convert to grayscale on GPU\n",
        "        flow_vectors = []\n",
        "        points_to_track = False # Flag if we have points from the previous frame\n",
        "\n",
        "        # --- Track existing points ---\n",
        "        if self.prev_gray_gpu is not None and self.prev_points_gpu is not None and not self.prev_points_gpu.empty():\n",
        "            points_to_track = True\n",
        "            # Ensure prev_points_gpu is float32 (required by LK flow)\n",
        "            if self.prev_points_gpu.type() != cv2.CV_32FC2:\n",
        "                 logger.warning(\"prev_points_gpu is not CV_32FC2, attempting conversion.\")\n",
        "                 try:\n",
        "                     temp_cpu = self.prev_points_gpu.download().astype(np.float32)\n",
        "                     temp_gpu = cv2.cuda_GpuMat()\n",
        "                     temp_gpu.upload(temp_cpu)\n",
        "                     self.prev_points_gpu = temp_gpu\n",
        "                     if self.prev_points_gpu.empty(): # Check if conversion failed\n",
        "                          points_to_track = False\n",
        "                          logger.error(\"Failed to convert prev_points_gpu to CV_32FC2.\")\n",
        "                 except Exception as e:\n",
        "                      points_to_track = False\n",
        "                      logger.error(f\"Error converting prev_points_gpu: {e}\")\n",
        "\n",
        "\n",
        "            if points_to_track:\n",
        "                # Calculate optical flow on GPU\n",
        "                next_points_gpu, status_gpu, err_gpu = self.gpu_lk_flow.calc(\n",
        "                    self.prev_gray_gpu, gray_gpu, self.prev_points_gpu, None\n",
        "                )\n",
        "\n",
        "                # Process results if calculation was successful\n",
        "                if next_points_gpu is not None and status_gpu is not None:\n",
        "                    status = status_gpu.download().flatten() # Download status vector\n",
        "                    # Filter points based on status BEFORE downloading coordinates for efficiency\n",
        "                    # Note: Indexing GpuMat directly like this requires newer OpenCV versions\n",
        "                    try:\n",
        "                        prev_points_gpu_filtered = self.prev_points_gpu[status == 1]\n",
        "                        next_points_gpu_filtered = next_points_gpu[status == 1]\n",
        "                    except cv2.error as e:\n",
        "                        # Fallback for older OpenCV: download all, then filter\n",
        "                        logger.warning(f\"GPU Mat indexing failed (likely older OpenCV): {e}. Using CPU filtering.\")\n",
        "                        prev_pts_cpu = self.prev_points_gpu.download().reshape(-1, 2)\n",
        "                        next_pts_cpu = next_points_gpu.download().reshape(-1, 2)\n",
        "                        good_old_cpu = prev_pts_cpu[status == 1]\n",
        "                        good_new_cpu = next_pts_cpu[status == 1]\n",
        "                        if len(good_new_cpu) > 0:\n",
        "                             prev_points_gpu_filtered = cv2.cuda_GpuMat()\n",
        "                             prev_points_gpu_filtered.upload(good_old_cpu.astype(np.float32).reshape(-1, 1, 2))\n",
        "                             next_points_gpu_filtered = cv2.cuda_GpuMat()\n",
        "                             next_points_gpu_filtered.upload(good_new_cpu.astype(np.float32).reshape(-1, 1, 2))\n",
        "                        else:\n",
        "                             prev_points_gpu_filtered = cv2.cuda_GpuMat() # Empty\n",
        "                             next_points_gpu_filtered = cv2.cuda_GpuMat() # Empty\n",
        "\n",
        "\n",
        "                    # Download filtered points if any survived\n",
        "                    if not next_points_gpu_filtered.empty() and not prev_points_gpu_filtered.empty():\n",
        "                        good_new = next_points_gpu_filtered.download().reshape(-1, 2)\n",
        "                        good_old = prev_points_gpu_filtered.download().reshape(-1, 2)\n",
        "\n",
        "                        # Create flow vectors (start_pt, end_pt)\n",
        "                        flow_vectors = [(tuple(map(int, p)), tuple(map(int, q))) for p, q in zip(good_old, good_new)]\n",
        "                        # Update previous points to the successfully tracked new points\n",
        "                        self.prev_points_gpu = next_points_gpu_filtered\n",
        "                    else:\n",
        "                        points_to_track = False # Lost all points\n",
        "                        self.prev_points_gpu = None # Reset points\n",
        "                else:\n",
        "                    points_to_track = False # Calculation failed\n",
        "                    self.prev_points_gpu = None\n",
        "\n",
        "        # --- Detect new features if needed ---\n",
        "        # Detect if no points were tracked or if point count dropped significantly\n",
        "        detect_new = False\n",
        "        if not points_to_track:\n",
        "            detect_new = True\n",
        "        elif self.prev_points_gpu is not None and self.prev_points_gpu.rows() < self.cpu_feature_params['maxCorners'] * 0.5:\n",
        "             detect_new = True\n",
        "\n",
        "\n",
        "        if detect_new:\n",
        "            # logger.debug(\"Detecting new features (GPU)...\")\n",
        "            # Detect good features to track on the GPU\n",
        "            detected_points_gpu_mat = self.gpu_detector.detect(gray_gpu, None) # Mask is None\n",
        "\n",
        "            # Process detected features\n",
        "            if detected_points_gpu_mat is not None and not detected_points_gpu_mat.empty():\n",
        "                 # Ensure the detected points are in the correct format (N x 1 x 2, float32) for LK flow\n",
        "                 self.prev_points_gpu = detected_points_gpu_mat.reshape(-1, 1, 2)\n",
        "                 if self.prev_points_gpu.type() != cv2.CV_32FC2:\n",
        "                     # Convert if necessary\n",
        "                     try:\n",
        "                         temp_cpu = self.prev_points_gpu.download().astype(np.float32)\n",
        "                         temp_gpu = cv2.cuda_GpuMat()\n",
        "                         temp_gpu.upload(temp_cpu)\n",
        "                         self.prev_points_gpu = temp_gpu\n",
        "                     except Exception as e:\n",
        "                          self.prev_points_gpu = None # Reset if conversion fails\n",
        "                          logger.error(f\"Error converting detected GPU points: {e}\")\n",
        "\n",
        "                 # logger.debug(f\"Detected {self.prev_points_gpu.rows() if self.prev_points_gpu is not None else 0} new features (GPU).\")\n",
        "            else:\n",
        "                self.prev_points_gpu = None # No features found\n",
        "                # logger.debug(\"No new features detected (GPU).\")\n",
        "\n",
        "\n",
        "        # Update previous frame's grayscale image\n",
        "        self.prev_gray_gpu = gray_gpu\n",
        "        # Emit calculated flow vectors\n",
        "        if flow_vectors:\n",
        "            self.flow_ready.emit(flow_vectors)\n",
        "\n",
        "\n",
        "    def run_cpu(self, frame):\n",
        "        \"\"\"Calculates optical flow using CPU.\"\"\"\n",
        "        gray_cpu = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # Convert to grayscale\n",
        "        flow_vectors = []\n",
        "        points_to_track = False\n",
        "\n",
        "        # --- Track existing points ---\n",
        "        if self.prev_gray_cpu is not None and self.prev_points_cpu is not None and len(self.prev_points_cpu) > 0:\n",
        "            points_to_track = True\n",
        "            # Calculate optical flow using Lucas-Kanade method\n",
        "            next_points_cpu, status, err = cv2.calcOpticalFlowPyrLK(\n",
        "                self.prev_gray_cpu, gray_cpu, self.prev_points_cpu, None, **self.cpu_lk_params\n",
        "            )\n",
        "\n",
        "            # Filter points based on status\n",
        "            if next_points_cpu is not None and status is not None:\n",
        "                good_new = next_points_cpu[status == 1]\n",
        "                good_old = self.prev_points_cpu[status == 1]\n",
        "\n",
        "                if len(good_new) > 0:\n",
        "                     # Create flow vectors\n",
        "                     flow_vectors = [(tuple(map(int, p)), tuple(map(int, q))) for p, q in zip(good_old, good_new)]\n",
        "                     # Update previous points to the successfully tracked new points\n",
        "                     self.prev_points_cpu = good_new.reshape(-1, 1, 2)\n",
        "                else:\n",
        "                    points_to_track = False # Lost all points\n",
        "                    self.prev_points_cpu = None # Reset points\n",
        "            else:\n",
        "                points_to_track = False # Calculation failed\n",
        "                self.prev_points_cpu = None\n",
        "\n",
        "\n",
        "        # --- Detect new features if needed ---\n",
        "        detect_new = False\n",
        "        if not points_to_track:\n",
        "            detect_new = True\n",
        "        elif self.prev_points_cpu is not None and len(self.prev_points_cpu) < self.cpu_feature_params['maxCorners'] * 0.5:\n",
        "            detect_new = True\n",
        "\n",
        "        if detect_new:\n",
        "            # logger.debug(\"Detecting new features (CPU)...\")\n",
        "            # Detect good features to track on CPU\n",
        "            self.prev_points_cpu = cv2.goodFeaturesToTrack(gray_cpu, mask=None, **self.cpu_feature_params)\n",
        "            # if self.prev_points_cpu is not None:\n",
        "            #      logger.debug(f\"Detected {len(self.prev_points_cpu)} new features (CPU).\")\n",
        "            # else:\n",
        "            #      logger.debug(\"No new features detected (CPU).\")\n",
        "\n",
        "\n",
        "        # Update previous frame's grayscale image\n",
        "        self.prev_gray_cpu = gray_cpu.copy()\n",
        "        # Emit calculated flow vectors\n",
        "        if flow_vectors:\n",
        "            self.flow_ready.emit(flow_vectors)\n",
        "\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Signals the thread to stop running.\"\"\"\n",
        "        self.running = False\n",
        "\n",
        "    def reset_state(self):\n",
        "        \"\"\"Resets the internal state (previous frame, points) for both CPU and GPU.\"\"\"\n",
        "        self.prev_gray_gpu = None\n",
        "        self.prev_points_gpu = None\n",
        "        self.prev_gray_cpu = None\n",
        "        self.prev_points_cpu = None\n",
        "        # logger.debug(\"Optical flow state reset.\")\n",
        "\n",
        "    @Slot(bool)\n",
        "    def set_enabled(self, enabled):\n",
        "        \"\"\"Enables or disables processing in the thread.\"\"\"\n",
        "        if self._enabled != enabled:\n",
        "            self._enabled = enabled\n",
        "            logger.info(f\"Optical Flow {'enabled' if enabled else 'disabled'}\")\n",
        "            if not enabled:\n",
        "                # Reset state when disabling to clear old points/frames\n",
        "                self.reset_state()\n",
        "\n",
        "    def get_processing_time(self):\n",
        "        \"\"\"Returns the processing time of the last calculation.\"\"\"\n",
        "        return self.processing_time_ms\n",
        "\n",
        "\n",
        "# --- Depth Estimation Thread ---\n",
        "class DepthEstimationThread(QThread):\n",
        "    \"\"\"Performs depth estimation using a MiDaS model (GPU if available).\"\"\"\n",
        "    depth_ready = Signal(np.ndarray) # Emits normalized depth map (0-1, CPU numpy array)\n",
        "    status_update = Signal(str)\n",
        "\n",
        "    def __init__(self, model_type=\"MiDaS_small\"): # Default to small model\n",
        "        super().__init__()\n",
        "        self.model_type = model_type\n",
        "        self.model = None\n",
        "        self.transform = None # Preprocessing transform\n",
        "        self.running = False\n",
        "        self._enabled = False # Disabled by default\n",
        "        self._frame_lock = QMutex()\n",
        "        self.current_frame = None\n",
        "        self.device = None # PyTorch device\n",
        "        self.processing_time_ms = 0.0\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Loads the MiDaS model and corresponding transform.\"\"\"\n",
        "        self.status_update.emit(f\"Loading MiDaS model: {self.model_type}...\")\n",
        "        logger.info(f\"Attempting to load MiDaS model: {self.model_type}\")\n",
        "        try:\n",
        "            # Determine device\n",
        "            if torch.cuda.is_available():\n",
        "                self.device = torch.device(\"cuda\")\n",
        "                logger.info(\"Using GPU for MiDaS.\")\n",
        "            else:\n",
        "                self.device = torch.device(\"cpu\")\n",
        "                logger.info(\"Using CPU for MiDaS.\")\n",
        "\n",
        "            # Load model and transforms from PyTorch Hub (requires internet connection on first run)\n",
        "            # Use trust_repo=True if needed for newer versions or custom models\n",
        "            self.model = torch.hub.load(\"intel-isl/MiDaS\", self.model_type, trust_repo=True)\n",
        "            midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\", trust_repo=True)\n",
        "\n",
        "            # Select the appropriate transform based on the model type\n",
        "            if self.model_type == \"MiDaS_small\":\n",
        "                 self.transform = midas_transforms.small_transform\n",
        "            elif \"dpt_large\" in self.model_type.lower() or \"dpt_hybrid\" in self.model_type.lower():\n",
        "                 self.transform = midas_transforms.dpt_transform # DPT models use dpt_transform\n",
        "            else: # Fallback for other MiDaS v2.1 models\n",
        "                 self.transform = midas_transforms.dpt_transform\n",
        "\n",
        "            self.model.to(self.device) # Move model to the selected device\n",
        "            self.model.eval() # Set model to evaluation mode (important for inference)\n",
        "            logger.info(f\"MiDaS model '{self.model_type}' loaded on {self.device}.\")\n",
        "            self.status_update.emit(f\"MiDaS model '{self.model_type}' loaded on {self.device}.\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Failed to load MiDaS model '{self.model_type}': {e}\"\n",
        "            logger.error(error_msg, exc_info=True)\n",
        "            self.status_update.emit(error_msg)\n",
        "            self.model = None\n",
        "            self.transform = None\n",
        "            self.device = None\n",
        "            return False\n",
        "\n",
        "    @Slot(np.ndarray, float) # Accept timestamp, though not directly used\n",
        "    def set_frame(self, frame, timestamp):\n",
        "        \"\"\"Receives a new frame for processing (thread-safe).\"\"\"\n",
        "        with QMutexLocker(self._frame_lock):\n",
        "            self.current_frame = frame.copy()\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Main processing loop for depth estimation.\"\"\"\n",
        "        if not self.load_model(): # Load model first\n",
        "            self.running = False\n",
        "            return\n",
        "\n",
        "        self.running = True\n",
        "        logger.info(\"Depth Estimation thread started.\")\n",
        "        while self.running:\n",
        "            if self._enabled: # Only process if enabled\n",
        "                frame_to_process = None\n",
        "                # Safely get the latest frame\n",
        "                with QMutexLocker(self._frame_lock):\n",
        "                    if self.current_frame is not None:\n",
        "                        frame_to_process = self.current_frame\n",
        "                        self.current_frame = None # Consume frame\n",
        "\n",
        "                if frame_to_process is not None and self.model is not None and self.transform is not None:\n",
        "                    start_time = time.perf_counter()\n",
        "                    try:\n",
        "                        # --- Preprocessing ---\n",
        "                        # Ensure input is RGB for MiDaS transforms (OpenCV uses BGR)\n",
        "                        if frame_to_process.shape[2] == 3: # BGR\n",
        "                            img_rgb = cv2.cvtColor(frame_to_process, cv2.COLOR_BGR2RGB)\n",
        "                        else:\n",
        "                             logger.warning(\"Depth estimation requires BGR input frame.\")\n",
        "                             continue # Skip if not 3 channels\n",
        "\n",
        "                        # Apply the appropriate transform and move to device\n",
        "                        input_batch = self.transform(img_rgb).to(self.device)\n",
        "\n",
        "                        # --- Inference ---\n",
        "                        with torch.no_grad(): # Disable gradient calculation for inference\n",
        "                            prediction = self.model(input_batch)\n",
        "\n",
        "                            # Resize prediction to original image size for visualization\n",
        "                            prediction = torch.nn.functional.interpolate(\n",
        "                                prediction.unsqueeze(1), # Add channel dimension\n",
        "                                size=img_rgb.shape[:2], # Target height, width\n",
        "                                mode=\"bicubic\",        # Use bicubic interpolation for smoother results\n",
        "                                align_corners=False,   # Recommended setting\n",
        "                            ).squeeze() # Remove channel dimension\n",
        "\n",
        "                        # --- Postprocessing ---\n",
        "                        # Move depth map to CPU and normalize to 0-1 range\n",
        "                        depth_map = prediction.cpu().numpy()\n",
        "                        normalized_depth = cv2.normalize(depth_map, None, 0, 1, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "\n",
        "                        # Emit the normalized depth map\n",
        "                        self.depth_ready.emit(normalized_depth)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Depth estimation error: {e}\", exc_info=True)\n",
        "                        self.status_update.emit(f\"Depth estimation error: {e}\")\n",
        "                    finally:\n",
        "                        end_time = time.perf_counter()\n",
        "                        self.processing_time_ms = (end_time - start_time) * 1000\n",
        "                else:\n",
        "                    self.msleep(5) # No frame, sleep briefly\n",
        "            else:\n",
        "                 # Disabled: Clear potential pending frame\n",
        "                 with QMutexLocker(self._frame_lock):\n",
        "                      self.current_frame = None\n",
        "                 self.msleep(50) # Sleep longer when disabled\n",
        "\n",
        "        # Cleanup\n",
        "        logger.info(\"Depth Estimation thread stopped.\")\n",
        "        self.model = None\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Signals the thread to stop running.\"\"\"\n",
        "        self.running = False\n",
        "\n",
        "    @Slot(bool)\n",
        "    def set_enabled(self, enabled):\n",
        "        \"\"\"Enables or disables processing in the thread.\"\"\"\n",
        "        if self._enabled != enabled:\n",
        "            self._enabled = enabled\n",
        "            logger.info(f\"Depth Estimation {'enabled' if enabled else 'disabled'}\")\n",
        "            if not enabled:\n",
        "                 # Clear potential pending frame when disabled\n",
        "                 with QMutexLocker(self._frame_lock):\n",
        "                      self.current_frame = None\n",
        "\n",
        "    def get_processing_time(self):\n",
        "        \"\"\"Returns the processing time of the last inference.\"\"\"\n",
        "        return self.processing_time_ms\n",
        "\n",
        "\n",
        "# --- Overlay Widget ---\n",
        "class OverlayWidget(QWidget):\n",
        "    \"\"\"Displays the transparent overlay with detections, paths, trajectory, etc.\"\"\"\n",
        "    def __init__(self, monitor_rect, parent=None):\n",
        "        super().__init__(parent)\n",
        "        # Configure window flags for a transparent, always-on-top overlay\n",
        "        self.setWindowFlags(Qt.FramelessWindowHint | Qt.WindowStaysOnTopHint | Qt.Tool)\n",
        "        self.setAttribute(Qt.WA_TranslucentBackground) # Enable transparency\n",
        "        self.setAttribute(Qt.WA_NoSystemBackground, True) # Don't draw default background\n",
        "        self.setAttribute(Qt.WA_PaintOnScreen) # Optimization hint\n",
        "\n",
        "        # Set geometry to match the target monitor\n",
        "        self.monitor_rect = monitor_rect\n",
        "        self.setGeometry(monitor_rect.left(), monitor_rect.top(), monitor_rect.width(), monitor_rect.height())\n",
        "\n",
        "        # --- Font Setup ---\n",
        "        # Attempt to load custom font, fallback to monospace\n",
        "        font_id = QFontDatabase.addApplicationFont(\"PressStart2P-Regular.ttf\") # Ensure font file is present\n",
        "        if font_id != -1:\n",
        "            font_families = QFontDatabase.applicationFontFamilies(font_id)\n",
        "            if font_families:\n",
        "                self.hud_font_name = font_families[0]\n",
        "                logger.info(f\"Loaded font: {self.hud_font_name}\")\n",
        "            else:\n",
        "                self.hud_font_name = FALLBACK_FONT\n",
        "                logger.warning(\"Could not get font family name from ID, using fallback.\")\n",
        "        else:\n",
        "            self.hud_font_name = FALLBACK_FONT\n",
        "            logger.warning(f\"Could not load font '{FONT_NAME}', using fallback '{FALLBACK_FONT}'.\")\n",
        "\n",
        "        self.font_small = QFont(self.hud_font_name, FONT_SIZE_SMALL)\n",
        "        self.font_medium = QFont(self.hud_font_name, FONT_SIZE_MEDIUM)\n",
        "\n",
        "        # --- Data Storage ---\n",
        "        self.detections = [] # List of (label, conf, box)\n",
        "        self.poses = []      # List of (keypoints_array, box, pose_conf)\n",
        "        self.flow_vectors = [] # List of (start_pt, end_pt)\n",
        "        self.depth_map = None  # Normalized depth map (numpy array)\n",
        "        self.hud_texts = {}    # Dictionary for HUD key-value pairs\n",
        "\n",
        "        # --- Smoothing & Prediction Data ---\n",
        "        self.target_history = collections.deque(maxlen=PATH_HISTORY_LENGTH) # History of (timestamp, smoothed_pos)\n",
        "        self.velocity_history = collections.deque(maxlen=VELOCITY_HISTORY_LENGTH) # History of (timestamp, smoothed_vel)\n",
        "        self.smoothed_target_center = None # QPointF for smoothed crosshair position\n",
        "        self.smoothed_velocity = QPointF(0, 0) # QPointF for smoothed velocity vector\n",
        "        self.smoothed_acceleration = QPointF(0, 0) # QPointF for smoothed acceleration vector\n",
        "\n",
        "        # --- Drawing Flags ---\n",
        "        self.show_boxes = True\n",
        "        self.show_labels = True\n",
        "        self.show_paths = True\n",
        "        self.show_trajectory = True # Controls the new physics-based trajectory\n",
        "        self.show_crosshair = True\n",
        "        self.show_poses = True # Linked to pose estimation enable state\n",
        "        self.show_flow = False\n",
        "        self.show_depth = False\n",
        "        self.show_hud = True\n",
        "\n",
        "        # --- Scanline Effect ---\n",
        "        self.scanline_y = 0\n",
        "        self.scanline_timer = QTimer(self)\n",
        "        self.scanline_timer.timeout.connect(self.update_scanline)\n",
        "        self.scanline_timer.start(SCANLINE_SPEED_MS)\n",
        "\n",
        "        # --- HUD Update Timer ---\n",
        "        self.hud_update_timer = QTimer(self)\n",
        "        self.hud_update_timer.timeout.connect(self.update) # Trigger repaint for HUD updates\n",
        "        self.hud_update_timer.start(UPDATE_INTERVAL_MS)\n",
        "\n",
        "        # --- Performance Metrics ---\n",
        "        self.capture_fps = 0.0\n",
        "        self.detection_fps = 0.0\n",
        "        self.pose_fps = 0.0\n",
        "        self.flow_fps = 0.0\n",
        "        self.depth_fps = 0.0\n",
        "        self.last_capture_time = time.perf_counter()\n",
        "        self.last_detection_time = time.perf_counter()\n",
        "        self.last_pose_time = time.perf_counter()\n",
        "        self.last_flow_time = time.perf_counter()\n",
        "        self.last_depth_time = time.perf_counter()\n",
        "        self.detection_proc_time = 0.0\n",
        "        self.pose_proc_time = 0.0\n",
        "        self.flow_proc_time = 0.0 # Added flow proc time tracking\n",
        "        self.depth_proc_time = 0.0 # Added depth proc time tracking\n",
        "\n",
        "\n",
        "    def update_scanline(self):\n",
        "        \"\"\"Updates the position of the scanline effect and triggers repaint.\"\"\"\n",
        "        self.scanline_y = (self.scanline_y + 5) % self.height() # Move scanline down\n",
        "        self.update() # Request redraw\n",
        "\n",
        "    @Slot(list, float, float)\n",
        "    def update_detections(self, detections, timestamp, proc_time):\n",
        "        \"\"\"Receives detection results, updates target, smoothing, and triggers repaint.\"\"\"\n",
        "        self.detections = detections\n",
        "        self.detection_proc_time = proc_time\n",
        "\n",
        "        # --- Target Selection (Example: Largest 'person' box) ---\n",
        "        target_box = None\n",
        "        max_area = 0\n",
        "        person_detections = [d for d in detections if d[0] == 'person'] # Filter\n",
        "\n",
        "        if person_detections:\n",
        "            for label, conf, box in person_detections:\n",
        "                x1, y1, x2, y2 = box\n",
        "                area = (x2 - x1) * (y2 - y1)\n",
        "                if area > max_area:\n",
        "                    max_area = area\n",
        "                    target_box = box\n",
        "\n",
        "        # --- Position Smoothing ---\n",
        "        raw_target_center = None\n",
        "        if target_box:\n",
        "            x1, y1, x2, y2 = target_box\n",
        "            raw_target_center = QPointF((x1 + x2) / 2, (y1 + y2) / 2) # Calculate center\n",
        "\n",
        "        if raw_target_center:\n",
        "            if self.smoothed_target_center is None:\n",
        "                # Initialize smoothing on the first valid detection\n",
        "                self.smoothed_target_center = raw_target_center\n",
        "            else:\n",
        "                # Apply EMA for position: smooth = alpha * new + (1 - alpha) * old\n",
        "                self.smoothed_target_center = (TARGET_CENTER_SMOOTHING_FACTOR * raw_target_center +\n",
        "                                              (1.0 - TARGET_CENTER_SMOOTHING_FACTOR) * self.smoothed_target_center)\n",
        "\n",
        "            # --- Update Target Position History ---\n",
        "            self.target_history.append((timestamp, self.smoothed_target_center)) # Store smoothed center\n",
        "\n",
        "            # --- Velocity Calculation and Smoothing ---\n",
        "            if len(self.target_history) >= 2:\n",
        "                 # Use last two points for instantaneous velocity\n",
        "                 t1, p1 = self.target_history[-2]\n",
        "                 t2, p2 = self.target_history[-1] # Most recent point\n",
        "                 dt = t2 - t1\n",
        "                 if dt > 1e-6: # Avoid division by zero\n",
        "                      raw_velocity = (p2 - p1) / dt # Calculate velocity vector\n",
        "                      # Apply EMA for velocity\n",
        "                      self.smoothed_velocity = (VELOCITY_SMOOTHING_FACTOR * raw_velocity +\n",
        "                                                (1.0 - VELOCITY_SMOOTHING_FACTOR) * self.smoothed_velocity)\n",
        "\n",
        "                      # --- Update Velocity History ---\n",
        "                      self.velocity_history.append((timestamp, self.smoothed_velocity))\n",
        "\n",
        "                      # --- Acceleration Calculation and Smoothing ---\n",
        "                      if len(self.velocity_history) >= 2:\n",
        "                           vt1, v1 = self.velocity_history[-2]\n",
        "                           vt2, v2 = self.velocity_history[-1] # Most recent velocity\n",
        "                           vdt = vt2 - vt1\n",
        "                           if vdt > 1e-6:\n",
        "                                raw_acceleration = (v2 - v1) / vdt # Calculate acceleration vector\n",
        "                                # Apply EMA for acceleration\n",
        "                                self.smoothed_acceleration = (ACCEL_SMOOTHING_FACTOR * raw_acceleration +\n",
        "                                                             (1.0 - ACCEL_SMOOTHING_FACTOR) * self.smoothed_acceleration)\n",
        "                           # else: # If dt is too small, keep previous acceleration\n",
        "                           #     pass\n",
        "                      # else: # Not enough velocity history, assume zero acceleration\n",
        "                      #     self.smoothed_acceleration = QPointF(0, 0)\n",
        "\n",
        "                 # else: # If dt is too small, keep previous velocity and acceleration\n",
        "                 #     pass\n",
        "            # else: # Not enough position history, assume zero velocity and acceleration\n",
        "            #     self.smoothed_velocity = QPointF(0, 0)\n",
        "            #     self.smoothed_acceleration = QPointF(0, 0)\n",
        "            #     self.velocity_history.clear() # Clear velocity history too\n",
        "\n",
        "\n",
        "        else:\n",
        "            # No target detected this frame. Options:\n",
        "            # 1. Keep last known velocity/acceleration (causes prediction to continue linearly)\n",
        "            # 2. Decay velocity/acceleration towards zero (prediction slows down)\n",
        "            # 3. Clear smoothed position (crosshair disappears)\n",
        "            # Current approach: Keep last known values.\n",
        "            # self.smoothed_target_center = None # Option 3\n",
        "            pass\n",
        "\n",
        "\n",
        "        # --- FPS Calculation ---\n",
        "        now = time.perf_counter()\n",
        "        time_diff = now - self.last_detection_time\n",
        "        if time_diff > 0:\n",
        "            self.detection_fps = 1.0 / time_diff\n",
        "        self.last_detection_time = now\n",
        "\n",
        "        self.update() # Trigger repaint\n",
        "\n",
        "    @Slot(list, float, float)\n",
        "    def update_poses(self, poses, timestamp, proc_time):\n",
        "        \"\"\"Receives pose estimation results.\"\"\"\n",
        "        self.poses = poses\n",
        "        self.pose_proc_time = proc_time\n",
        "        # --- FPS Calculation ---\n",
        "        now = time.perf_counter()\n",
        "        time_diff = now - self.last_pose_time\n",
        "        if time_diff > 0:\n",
        "            self.pose_fps = 1.0 / time_diff\n",
        "        self.last_pose_time = now\n",
        "        if self.show_poses: # Only repaint if poses are visible\n",
        "             self.update()\n",
        "\n",
        "    @Slot(list)\n",
        "    def update_flow(self, flow_vectors):\n",
        "        \"\"\"Receives optical flow results.\"\"\"\n",
        "        self.flow_vectors = flow_vectors\n",
        "        # Get processing time from the thread if available (needs modification in OpticalFlowThread)\n",
        "        # self.flow_proc_time = self.sender().get_processing_time() # Example if thread emitted it\n",
        "        # --- FPS Calculation (Approximate based on signal arrival) ---\n",
        "        now = time.perf_counter()\n",
        "        time_diff = now - self.last_flow_time\n",
        "        if time_diff > 0:\n",
        "            self.flow_fps = 1.0 / time_diff\n",
        "        self.last_flow_time = now\n",
        "        if self.show_flow: # Only repaint if flow is visible\n",
        "            self.update()\n",
        "\n",
        "    @Slot(np.ndarray)\n",
        "    def update_depth(self, depth_map):\n",
        "        \"\"\"Receives depth estimation results.\"\"\"\n",
        "        self.depth_map = depth_map\n",
        "        # Get processing time from the thread if available\n",
        "        # self.depth_proc_time = self.sender().get_processing_time() # Example\n",
        "        # --- FPS Calculation (Approximate based on signal arrival) ---\n",
        "        now = time.perf_counter()\n",
        "        time_diff = now - self.last_depth_time\n",
        "        if time_diff > 0:\n",
        "            self.depth_fps = 1.0 / time_diff\n",
        "        self.last_depth_time = now\n",
        "        if self.show_depth: # Only repaint if depth is visible\n",
        "            self.update()\n",
        "\n",
        "    @Slot(dict)\n",
        "    def update_hud(self, hud_data):\n",
        "        \"\"\"Receives text data for the HUD.\"\"\"\n",
        "        self.hud_texts.update(hud_data)\n",
        "        # No repaint needed here, hud_update_timer handles periodic updates\n",
        "\n",
        "    @Slot(float)\n",
        "    def update_capture_fps(self, fps):\n",
        "        \"\"\"Receives capture FPS.\"\"\"\n",
        "        self.capture_fps = fps\n",
        "\n",
        "    # --- Toggling drawing elements ---\n",
        "    @Slot(bool)\n",
        "    def toggle_boxes(self, show): self.show_boxes = show; self.update()\n",
        "    @Slot(bool)\n",
        "    def toggle_labels(self, show): self.show_labels = show; self.update()\n",
        "    @Slot(bool)\n",
        "    def toggle_paths(self, show): self.show_paths = show; self.update()\n",
        "    @Slot(bool)\n",
        "    def toggle_trajectory(self, show): self.show_trajectory = show; self.update()\n",
        "    @Slot(bool)\n",
        "    def toggle_crosshair(self, show): self.show_crosshair = show; self.update()\n",
        "    @Slot(bool)\n",
        "    def toggle_poses(self, show): self.show_poses = show; self.update()\n",
        "    @Slot(bool)\n",
        "    def toggle_flow(self, show): self.show_flow = show; self.update()\n",
        "    @Slot(bool)\n",
        "    def toggle_depth(self, show): self.show_depth = show; self.update()\n",
        "    @Slot(bool)\n",
        "    def toggle_hud(self, show): self.show_hud = show; self.update()\n",
        "\n",
        "\n",
        "    def paintEvent(self, event):\n",
        "        \"\"\"Draws all overlay elements: depth, scanline, flow, detections, poses, path, trajectory, crosshair, HUD.\"\"\"\n",
        "        painter = QPainter(self)\n",
        "        painter.setRenderHint(QPainter.Antialiasing) # Enable anti-aliasing for smoother lines/curves\n",
        "\n",
        "        # --- Clear Background (Essential for transparency) ---\n",
        "        painter.fillRect(self.rect(), Qt.transparent)\n",
        "\n",
        "        # --- 1. Depth Map (Draw first, as background) ---\n",
        "        if self.show_depth and self.depth_map is not None:\n",
        "            try:\n",
        "                # Convert normalized float32 depth map (0-1) to 8-bit grayscale QImage\n",
        "                depth_8bit = (self.depth_map * 255).astype(np.uint8)\n",
        "                h, w = depth_8bit.shape\n",
        "                # Create QImage directly from numpy data (efficient)\n",
        "                q_image = QImage(depth_8bit.data, w, h, w, QImage.Format_Grayscale8)\n",
        "                pixmap = QPixmap.fromImage(q_image) # Convert to QPixmap for drawing\n",
        "                # Scale pixmap to fit widget size, maintaining aspect ratio\n",
        "                scaled_pixmap = pixmap.scaled(self.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation)\n",
        "                # Center the scaled pixmap within the widget\n",
        "                x_offset = (self.width() - scaled_pixmap.width()) / 2\n",
        "                y_offset = (self.height() - scaled_pixmap.height()) / 2\n",
        "                painter.drawPixmap(int(x_offset), int(y_offset), scaled_pixmap)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error drawing depth map: {e}\", exc_info=True)\n",
        "\n",
        "\n",
        "        # --- 2. Scanline Effect ---\n",
        "        painter.setPen(QPen(QColor(0, 255, 0, 50), 2)) # Semi-transparent green line\n",
        "        painter.drawLine(0, self.scanline_y, self.width(), self.scanline_y)\n",
        "\n",
        "        # --- 3. Optical Flow Vectors ---\n",
        "        if self.show_flow and self.flow_vectors:\n",
        "            painter.setPen(QPen(YELLOW_COLOR, 1)) # Thin yellow lines\n",
        "            for start_pt, end_pt in self.flow_vectors:\n",
        "                painter.drawLine(QPoint(*start_pt), QPoint(*end_pt))\n",
        "                # Optional: Draw small circles at the end points\n",
        "                # painter.setBrush(YELLOW_COLOR)\n",
        "                # painter.drawEllipse(QPoint(*end_pt), 1, 1)\n",
        "\n",
        "\n",
        "        # --- 4. Detections (Boxes and Labels) ---\n",
        "        if self.detections:\n",
        "            painter.setFont(self.font_small) # Use small font for labels\n",
        "            for label, confidence, box in self.detections:\n",
        "                x1, y1, x2, y2 = box\n",
        "                box_width = x2 - x1\n",
        "                box_height = y2 - y1\n",
        "\n",
        "                # Draw bounding box\n",
        "                if self.show_boxes:\n",
        "                    painter.setPen(QPen(GREEN_COLOR, 2)) # Green box outline\n",
        "                    painter.drawRect(x1, y1, box_width, box_height)\n",
        "\n",
        "                # Draw label text\n",
        "                if self.show_labels:\n",
        "                    painter.setPen(GREEN_COLOR) # Set text color\n",
        "                    # painter.setBrush(GREEN_COLOR) # Use Brush if filling text background\n",
        "                    text = f\"{label} ({confidence:.2f})\"\n",
        "                    # Position text slightly above the box\n",
        "                    painter.drawText(x1 + 2, y1 - 5, text)\n",
        "\n",
        "\n",
        "        # --- 5. Pose Estimation Skeletons ---\n",
        "        if self.show_poses and self.poses:\n",
        "             painter.setPen(QPen(CYAN_COLOR, 2)) # Default color for keypoints\n",
        "             for keypoints_data, box, pose_conf in self.poses:\n",
        "                  # keypoints_data is N x 3 (x, y, confidence)\n",
        "                  num_keypoints = keypoints_data.shape[0]\n",
        "                  points = [] # Store (index, QPointF) for visible keypoints\n",
        "\n",
        "                  # Draw Keypoints (small circles)\n",
        "                  for i in range(num_keypoints):\n",
        "                       x, y, conf = keypoints_data[i]\n",
        "                       # Only draw keypoints above the confidence threshold\n",
        "                       if conf > DEFAULT_POSE_CONFIDENCE_THRESHOLD:\n",
        "                            pt = QPointF(x, y)\n",
        "                            points.append((i, pt)) # Store index and point\n",
        "                            painter.setBrush(POSE_COLORS[i % len(POSE_COLORS)]) # Cycle through colors\n",
        "                            painter.setPen(Qt.NoPen) # No outline for points\n",
        "                            painter.drawEllipse(pt, 3, 3) # Draw 3x3 ellipse\n",
        "                       else:\n",
        "                            points.append((i, None)) # Mark invisible points\n",
        "\n",
        "                  # Draw Connections (Skeleton Lines)\n",
        "                  painter.setPen(QPen(MAGENTA_COLOR, 1)) # Color for skeleton lines\n",
        "                  # Create a dictionary for quick lookup of visible points by index\n",
        "                  points_dict = {idx: pt for idx, pt in points if pt is not None}\n",
        "                  # Iterate through predefined connections\n",
        "                  for i, (start_idx, end_idx) in enumerate(POSE_CONNECTIONS):\n",
        "                       # Draw line only if both start and end keypoints are visible\n",
        "                       if start_idx in points_dict and end_idx in points_dict:\n",
        "                            pt1 = points_dict[start_idx]\n",
        "                            pt2 = points_dict[end_idx]\n",
        "                            # Optional: Color lines based on connection index or body part\n",
        "                            # painter.setPen(QPen(POSE_COLORS[i % len(POSE_COLORS)], 1))\n",
        "                            painter.drawLine(pt1, pt2)\n",
        "\n",
        "\n",
        "        # --- 6. Target Path History ---\n",
        "        if self.show_paths and len(self.target_history) > 1:\n",
        "            painter.setPen(QPen(ORANGE_COLOR.lighter(120), 1)) # Lighter Orange, semi-transparent width 1\n",
        "            # Extract points from history (deque stores (timestamp, QPointF))\n",
        "            path_points = [p for t, p in self.target_history]\n",
        "            poly = QPolygonF(path_points) # Create polygon from points\n",
        "            painter.drawPolyline(poly) # Draw the path history\n",
        "\n",
        "\n",
        "        # --- 7. Predicted Trajectory (Physics-Based) ---\n",
        "        if self.show_trajectory and self.smoothed_target_center and len(self.velocity_history) >= 1:\n",
        "            painter.setPen(QPen(MAGENTA_COLOR, 2, Qt.DashLine)) # Magenta dashed line, width 2\n",
        "\n",
        "            # --- Physics Prediction Loop ---\n",
        "            predicted_points = [self.smoothed_target_center] # Start prediction from current smoothed position\n",
        "            current_pred_pos = self.smoothed_target_center\n",
        "            current_pred_vel = self.smoothed_velocity # Start with current smoothed velocity\n",
        "            current_pred_accel = self.smoothed_acceleration # Use current smoothed acceleration\n",
        "\n",
        "            num_steps = int(TRAJECTORY_PREDICTION_DURATION / PREDICTION_TIME_STEP)\n",
        "            dt = PREDICTION_TIME_STEP\n",
        "\n",
        "            for _ in range(num_steps):\n",
        "                # Kinematic equation: pos = pos_old + vel*dt + 0.5*accel*dt^2\n",
        "                delta_pos_vel = current_pred_vel * dt\n",
        "                delta_pos_accel = 0.5 * current_pred_accel * dt * dt\n",
        "                next_pred_pos = current_pred_pos + delta_pos_vel + delta_pos_accel\n",
        "\n",
        "                # Update velocity for the next step: vel = vel_old + accel*dt\n",
        "                current_pred_vel = current_pred_vel + current_pred_accel * dt\n",
        "\n",
        "                # Add the predicted point to the list\n",
        "                predicted_points.append(next_pred_pos)\n",
        "                # Update current position for the next iteration\n",
        "                current_pred_pos = next_pred_pos\n",
        "\n",
        "            # Draw the calculated trajectory path\n",
        "            if len(predicted_points) > 1:\n",
        "                painter.drawPolyline(QPolygonF(predicted_points))\n",
        "\n",
        "\n",
        "        # --- 8. Smoothed Crosshair ---\n",
        "        if self.show_crosshair and self.smoothed_target_center:\n",
        "            painter.setPen(QPen(RED_COLOR, 1)) # Thin red lines\n",
        "            # Draw crosshair at the smoothed target center\n",
        "            cx = int(self.smoothed_target_center.x())\n",
        "            cy = int(self.smoothed_target_center.y())\n",
        "            size = 10 # Length of crosshair lines from center\n",
        "            painter.drawLine(cx - size, cy, cx + size, cy) # Horizontal line\n",
        "            painter.drawLine(cx, cy - size, cx, cy + size) # Vertical line\n",
        "            # Optional: Draw a small circle at the very center\n",
        "            # painter.drawEllipse(self.smoothed_target_center, 2, 2)\n",
        "\n",
        "\n",
        "        # --- 9. HUD Text ---\n",
        "        if self.show_hud:\n",
        "            painter.setFont(self.font_medium) # Use medium font for HUD\n",
        "            painter.setPen(TEXT_COLOR) # Set text color\n",
        "            y_offset = 20 # Starting Y position for HUD text (from top)\n",
        "            line_height = 15 # Spacing between lines\n",
        "\n",
        "            # Display FPS and processing times\n",
        "            painter.drawText(10, y_offset, f\"Cap: {self.capture_fps:.1f} FPS\")\n",
        "            y_offset += line_height\n",
        "            painter.drawText(10, y_offset, f\"Det: {self.detection_fps:.1f} FPS ({self.detection_proc_time:.1f} ms)\")\n",
        "            y_offset += line_height\n",
        "            if self.show_poses: # Only show pose info if enabled\n",
        "                 painter.drawText(10, y_offset, f\"Pose: {self.pose_fps:.1f} FPS ({self.pose_proc_time:.1f} ms)\")\n",
        "                 y_offset += line_height\n",
        "            if self.show_flow: # Only show flow info if enabled\n",
        "                 # TODO: Get flow proc time from thread and display it\n",
        "                 painter.drawText(10, y_offset, f\"Flow: {self.flow_fps:.1f} FPS ({self.flow_proc_time:.1f} ms)\")\n",
        "                 y_offset += line_height\n",
        "            if self.show_depth: # Only show depth info if enabled\n",
        "                 # TODO: Get depth proc time from thread and display it\n",
        "                 painter.drawText(10, y_offset, f\"Depth: {self.depth_fps:.1f} FPS ({self.depth_proc_time:.1f} ms)\")\n",
        "                 y_offset += line_height\n",
        "\n",
        "            # Display system info from hud_texts dictionary\n",
        "            for key, value in self.hud_texts.items():\n",
        "                painter.drawText(10, y_offset, f\"{key}: {value}\")\n",
        "                y_offset += line_height\n",
        "\n",
        "            # Display Crosshair Smoothing Info in HUD\n",
        "            if self.show_crosshair and self.smoothed_target_center:\n",
        "                 sx, sy = int(self.smoothed_target_center.x()), int(self.smoothed_target_center.y())\n",
        "                 painter.drawText(10, y_offset, f\"Crosshair (Smooth): {sx}, {sy}\")\n",
        "                 y_offset += line_height\n",
        "                 # Display velocity and acceleration magnitude (optional debug info)\n",
        "                 vel_mag = (self.smoothed_velocity.x()**2 + self.smoothed_velocity.y()**2)**0.5\n",
        "                 acc_mag = (self.smoothed_acceleration.x()**2 + self.smoothed_acceleration.y()**2)**0.5\n",
        "                 painter.drawText(10, y_offset, f\"Vel: {vel_mag:.1f} px/s\")\n",
        "                 y_offset += line_height\n",
        "                 painter.drawText(10, y_offset, f\"Acc: {acc_mag:.1f} px/s^2\")\n",
        "                 y_offset += line_height\n",
        "\n",
        "\n",
        "        painter.end() # Finish painting\n",
        "\n",
        "\n",
        "# --- Main Application Window ---\n",
        "class MainWindow(QMainWindow):\n",
        "    \"\"\"Main application window with controls and thread management.\"\"\"\n",
        "    # --- Signals for Controlling Worker Threads ---\n",
        "    capture_interval_changed = Signal(int)\n",
        "    detection_enabled_changed = Signal(bool)\n",
        "    detection_conf_changed = Signal(float)\n",
        "    detection_nms_changed = Signal(float)\n",
        "    pose_enabled_changed = Signal(bool)\n",
        "    pose_conf_changed = Signal(float)\n",
        "    flow_enabled_changed = Signal(bool)\n",
        "    depth_enabled_changed = Signal(bool)\n",
        "\n",
        "    # --- Signals for Toggling Overlay Elements ---\n",
        "    toggle_boxes_signal = Signal(bool)\n",
        "    toggle_labels_signal = Signal(bool)\n",
        "    toggle_paths_signal = Signal(bool)\n",
        "    toggle_trajectory_signal = Signal(bool)\n",
        "    toggle_crosshair_signal = Signal(bool)\n",
        "    toggle_poses_signal = Signal(bool)\n",
        "    toggle_flow_signal = Signal(bool)\n",
        "    toggle_depth_signal = Signal(bool)\n",
        "    toggle_hud_signal = Signal(bool)\n",
        "\n",
        "    # --- Signals for Sending Data to Overlay ---\n",
        "    hud_data_signal = Signal(dict)\n",
        "    capture_fps_signal = Signal(float)\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.setWindowTitle(\"Real-Time Detection Overlay\")\n",
        "        self.setGeometry(100, 100, 450, 750) # Adjusted height slightly for new controls/info\n",
        "\n",
        "        # --- Monitor Selection ---\n",
        "        try:\n",
        "            self.monitors = mss.mss().monitors[1:] # Exclude the 'all monitors' entry (index 0)\n",
        "            if not self.monitors:\n",
        "                raise RuntimeError(\"No monitors found (excluding 'all monitors').\")\n",
        "            self.selected_monitor_spec = self.monitors[0] # Default to the first physical monitor\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to get monitor list: {e}. Exiting.\", exc_info=True)\n",
        "            # Show error message box\n",
        "            msg_box = QtWidgets.QMessageBox()\n",
        "            msg_box.setIcon(QtWidgets.QMessageBox.Critical)\n",
        "            msg_box.setText(f\"Error initializing monitors: {e}\\n\\nPlease ensure display drivers are working.\")\n",
        "            msg_box.setWindowTitle(\"Monitor Error\")\n",
        "            msg_box.exec()\n",
        "            sys.exit(1) # Exit if monitors can't be found\n",
        "\n",
        "\n",
        "        # --- Central Widget and Layout ---\n",
        "        self.central_widget = QWidget()\n",
        "        self.main_layout = QVBoxLayout(self.central_widget)\n",
        "        self.setCentralWidget(self.central_widget)\n",
        "\n",
        "        # --- Control Panel ---\n",
        "        self.control_group = QGroupBox(\"Controls\")\n",
        "        self.control_layout = QGridLayout()\n",
        "        self.control_group.setLayout(self.control_layout)\n",
        "        self.main_layout.addWidget(self.control_group)\n",
        "\n",
        "        # Monitor Selection Dropdown\n",
        "        self.monitor_combo = QComboBox()\n",
        "        self.monitor_combo.addItems([f\"Monitor {i+1} ({m['width']}x{m['height']}@{m['left']},{m['top']})\" for i, m in enumerate(self.monitors)])\n",
        "        self.monitor_combo.currentIndexChanged.connect(self.update_monitor)\n",
        "        self.control_layout.addWidget(QLabel(\"Target Monitor:\"), 0, 0)\n",
        "        self.control_layout.addWidget(self.monitor_combo, 0, 1, 1, 2) # Span 2 columns\n",
        "\n",
        "        # Capture Interval SpinBox\n",
        "        self.interval_spinbox = QSpinBox()\n",
        "        self.interval_spinbox.setRange(1, 1000) # 1ms to 1s\n",
        "        self.interval_spinbox.setValue(CAPTURE_INTERVAL_MS)\n",
        "        self.interval_spinbox.setSuffix(\" ms\")\n",
        "        self.interval_spinbox.setToolTip(\"Interval between screen captures (lower = higher FPS target)\")\n",
        "        self.interval_spinbox.valueChanged.connect(self.capture_interval_changed.emit)\n",
        "        self.control_layout.addWidget(QLabel(\"Capture Interval:\"), 1, 0)\n",
        "        self.control_layout.addWidget(self.interval_spinbox, 1, 1, 1, 2)\n",
        "\n",
        "        # --- Detection Controls Group ---\n",
        "        self.detection_group = QGroupBox(\"Object Detection (YOLOv8n)\")\n",
        "        self.detection_layout = QGridLayout()\n",
        "        self.detection_group.setLayout(self.detection_layout)\n",
        "        self.main_layout.addWidget(self.detection_group)\n",
        "\n",
        "        self.detection_enabled_check = QCheckBox(\"Enable Detection\")\n",
        "        self.detection_enabled_check.setChecked(True) # Enabled by default\n",
        "        self.detection_enabled_check.toggled.connect(self.detection_enabled_changed.emit)\n",
        "        self.detection_layout.addWidget(self.detection_enabled_check, 0, 0, 1, 3) # Span columns\n",
        "\n",
        "        self.conf_spinbox = QDoubleSpinBox()\n",
        "        self.conf_spinbox.setRange(0.01, 1.0)\n",
        "        self.conf_spinbox.setSingleStep(0.05)\n",
        "        self.conf_spinbox.setValue(DEFAULT_CONFIDENCE_THRESHOLD)\n",
        "        self.conf_spinbox.setToolTip(\"Minimum confidence score for detected objects\")\n",
        "        self.conf_spinbox.valueChanged.connect(self.detection_conf_changed.emit)\n",
        "        self.detection_layout.addWidget(QLabel(\"Confidence Threshold:\"), 1, 0)\n",
        "        self.detection_layout.addWidget(self.conf_spinbox, 1, 1, 1, 2)\n",
        "\n",
        "        self.nms_spinbox = QDoubleSpinBox()\n",
        "        self.nms_spinbox.setRange(0.01, 1.0)\n",
        "        self.nms_spinbox.setSingleStep(0.05)\n",
        "        self.nms_spinbox.setValue(DEFAULT_NMS_THRESHOLD)\n",
        "        self.nms_spinbox.setToolTip(\"Non-Maximum Suppression (NMS) threshold (IoU)\")\n",
        "        self.nms_spinbox.valueChanged.connect(self.detection_nms_changed.emit)\n",
        "        self.detection_layout.addWidget(QLabel(\"NMS Threshold:\"), 2, 0)\n",
        "        self.detection_layout.addWidget(self.nms_spinbox, 2, 1, 1, 2)\n",
        "\n",
        "        # --- Pose Estimation Controls Group ---\n",
        "        self.pose_group = QGroupBox(\"Pose Estimation (YOLOv8n-Pose)\")\n",
        "        self.pose_layout = QGridLayout()\n",
        "        self.pose_group.setLayout(self.pose_layout)\n",
        "        self.main_layout.addWidget(self.pose_group)\n",
        "\n",
        "        self.pose_enabled_check = QCheckBox(\"Enable Pose Estimation\")\n",
        "        self.pose_enabled_check.setChecked(False) # Default disabled\n",
        "        self.pose_enabled_check.toggled.connect(self.pose_enabled_changed.emit)\n",
        "        self.pose_enabled_check.toggled.connect(self.toggle_poses_signal.emit) # Link enable state to drawing toggle\n",
        "        self.pose_layout.addWidget(self.pose_enabled_check, 0, 0, 1, 3)\n",
        "\n",
        "        self.pose_conf_spinbox = QDoubleSpinBox()\n",
        "        self.pose_conf_spinbox.setRange(0.01, 1.0)\n",
        "        self.pose_conf_spinbox.setSingleStep(0.05)\n",
        "        self.pose_conf_spinbox.setValue(DEFAULT_POSE_CONFIDENCE_THRESHOLD)\n",
        "        self.pose_conf_spinbox.setToolTip(\"Minimum confidence score for detected poses/keypoints\")\n",
        "        self.pose_conf_spinbox.valueChanged.connect(self.pose_conf_changed.emit)\n",
        "        self.pose_layout.addWidget(QLabel(\"Pose Conf Threshold:\"), 1, 0)\n",
        "        self.pose_layout.addWidget(self.pose_conf_spinbox, 1, 1, 1, 2)\n",
        "\n",
        "\n",
        "        # --- Other Feature Controls Group ---\n",
        "        self.features_group = QGroupBox(\"Other Features\")\n",
        "        self.features_layout = QGridLayout()\n",
        "        self.features_group.setLayout(self.features_layout)\n",
        "        self.main_layout.addWidget(self.features_group)\n",
        "\n",
        "        self.flow_enabled_check = QCheckBox(\"Enable Optical Flow\")\n",
        "        self.flow_enabled_check.setChecked(False) # Default disabled\n",
        "        self.flow_enabled_check.toggled.connect(self.flow_enabled_changed.emit)\n",
        "        self.flow_enabled_check.toggled.connect(self.toggle_flow_signal.emit) # Link enable to drawing\n",
        "        self.features_layout.addWidget(self.flow_enabled_check, 0, 0)\n",
        "\n",
        "        self.depth_enabled_check = QCheckBox(\"Enable Depth Estimation\")\n",
        "        self.depth_enabled_check.setChecked(False) # Default disabled\n",
        "        self.depth_enabled_check.toggled.connect(self.depth_enabled_changed.emit)\n",
        "        self.depth_enabled_check.toggled.connect(self.toggle_depth_signal.emit) # Link enable to drawing\n",
        "        self.features_layout.addWidget(self.depth_enabled_check, 0, 1)\n",
        "\n",
        "\n",
        "        # --- Overlay Visibility Controls Group ---\n",
        "        self.visibility_group = QGroupBox(\"Overlay Visibility\")\n",
        "        self.visibility_layout = QGridLayout()\n",
        "        self.visibility_group.setLayout(self.visibility_layout)\n",
        "        self.main_layout.addWidget(self.visibility_group)\n",
        "\n",
        "        # Add checkboxes for each visual element\n",
        "        self.show_boxes_check = QCheckBox(\"Show Boxes\")\n",
        "        self.show_boxes_check.setChecked(True)\n",
        "        self.show_boxes_check.toggled.connect(self.toggle_boxes_signal.emit)\n",
        "        self.visibility_layout.addWidget(self.show_boxes_check, 0, 0)\n",
        "\n",
        "        self.show_labels_check = QCheckBox(\"Show Labels\")\n",
        "        self.show_labels_check.setChecked(True)\n",
        "        self.show_labels_check.toggled.connect(self.toggle_labels_signal.emit)\n",
        "        self.visibility_layout.addWidget(self.show_labels_check, 0, 1)\n",
        "\n",
        "        self.show_paths_check = QCheckBox(\"Show Path History\") # Renamed for clarity\n",
        "        self.show_paths_check.setChecked(True)\n",
        "        self.show_paths_check.toggled.connect(self.toggle_paths_signal.emit)\n",
        "        self.visibility_layout.addWidget(self.show_paths_check, 1, 0)\n",
        "\n",
        "        self.show_trajectory_check = QCheckBox(\"Show Trajectory\")\n",
        "        self.show_trajectory_check.setChecked(True)\n",
        "        self.show_trajectory_check.toggled.connect(self.toggle_trajectory_signal.emit)\n",
        "        self.visibility_layout.addWidget(self.show_trajectory_check, 1, 1)\n",
        "\n",
        "        self.show_crosshair_check = QCheckBox(\"Show Crosshair\")\n",
        "        self.show_crosshair_check.setChecked(True)\n",
        "        self.show_crosshair_check.toggled.connect(self.toggle_crosshair_signal.emit)\n",
        "        self.visibility_layout.addWidget(self.show_crosshair_check, 2, 0)\n",
        "\n",
        "        # Note: Pose visibility is implicitly controlled by the \"Enable Pose Estimation\" checkbox\n",
        "        # via the toggle_poses_signal connection. A separate checkbox here would be redundant.\n",
        "\n",
        "        self.show_hud_check = QCheckBox(\"Show HUD\")\n",
        "        self.show_hud_check.setChecked(True)\n",
        "        self.show_hud_check.toggled.connect(self.toggle_hud_signal.emit)\n",
        "        self.visibility_layout.addWidget(self.show_hud_check, 2, 1)\n",
        "\n",
        "\n",
        "        # --- Log Output Area ---\n",
        "        self.log_group = QGroupBox(\"Log Output\")\n",
        "        self.log_layout = QVBoxLayout()\n",
        "        self.log_group.setLayout(self.log_layout)\n",
        "        self.log_text_edit = QTextEdit()\n",
        "        self.log_text_edit.setReadOnly(True)\n",
        "        self.log_text_edit.setFont(QFont(\"Monospace\", 8)) # Smaller monospace font for logs\n",
        "        self.log_layout.addWidget(self.log_text_edit)\n",
        "        self.main_layout.addWidget(self.log_group)\n",
        "        self.main_layout.setStretchFactor(self.log_group, 1) # Allow log area to expand\n",
        "\n",
        "        # --- Status Bar ---\n",
        "        self.status_bar = self.statusBar()\n",
        "        self.status_bar.showMessage(\"Ready\")\n",
        "\n",
        "        # --- Initialize Overlay Widget ---\n",
        "        monitor_qrect = QRect(\n",
        "            self.selected_monitor_spec['left'],\n",
        "            self.selected_monitor_spec['top'],\n",
        "            self.selected_monitor_spec['width'],\n",
        "            self.selected_monitor_spec['height']\n",
        "        )\n",
        "        self.overlay = OverlayWidget(monitor_qrect)\n",
        "        self.overlay.show() # Show the overlay window\n",
        "\n",
        "        # --- Setup Logging Handler ---\n",
        "        self.log_handler = QTextEditLogger(self.log_text_edit)\n",
        "        logging.getLogger().addHandler(self.log_handler) # Add handler to root logger\n",
        "        logging.getLogger().setLevel(logging.INFO) # Set logging level\n",
        "\n",
        "        # --- System Info Timer ---\n",
        "        self.system_info_timer = QTimer(self)\n",
        "        self.system_info_timer.timeout.connect(self.update_system_info)\n",
        "        self.system_info_timer.start(SYSTEM_INFO_INTERVAL_MS)\n",
        "\n",
        "        # --- Initialize Threads ---\n",
        "        self.capture_thread = None\n",
        "        self.detection_thread = None\n",
        "        self.pose_thread = None\n",
        "        self.flow_thread = None\n",
        "        self.depth_thread = None\n",
        "        self.start_threads() # Start worker threads\n",
        "\n",
        "        # --- Connect Signals and Slots ---\n",
        "        self.connect_signals()\n",
        "\n",
        "        # --- Initial System Info Update ---\n",
        "        self.update_system_info() # Populate HUD immediately\n",
        "\n",
        "\n",
        "    def update_monitor(self, index):\n",
        "        \"\"\"Handles monitor selection changes by restarting threads.\"\"\"\n",
        "        if 0 <= index < len(self.monitors):\n",
        "            self.selected_monitor_spec = self.monitors[index]\n",
        "            logger.info(f\"Monitor changed to: {index+1} - {self.selected_monitor_spec}\")\n",
        "            self.status_bar.showMessage(f\"Monitor changed to {index+1}. Restarting capture...\")\n",
        "\n",
        "            # 1. Stop existing threads gracefully\n",
        "            self.stop_threads()\n",
        "\n",
        "            # 2. Update overlay geometry and internal reference\n",
        "            monitor_qrect = QRect(\n",
        "                self.selected_monitor_spec['left'],\n",
        "                self.selected_monitor_spec['top'],\n",
        "                self.selected_monitor_spec['width'],\n",
        "                self.selected_monitor_spec['height']\n",
        "            )\n",
        "            self.overlay.monitor_rect = monitor_qrect # Update internal rect\n",
        "            self.overlay.setGeometry(monitor_qrect) # Resize/reposition overlay window\n",
        "\n",
        "            # 3. Restart threads with the new monitor spec\n",
        "            self.start_threads()\n",
        "\n",
        "            # 4. Reconnect signals as threads are new instances\n",
        "            self.connect_signals() # Crucial step!\n",
        "\n",
        "            self.status_bar.showMessage(f\"Capture restarted on monitor {index+1}.\")\n",
        "        else:\n",
        "             logger.error(f\"Invalid monitor index selected: {index}\")\n",
        "\n",
        "\n",
        "    def start_threads(self):\n",
        "        \"\"\"Initializes and starts all worker threads.\"\"\"\n",
        "        logger.info(\"Starting worker threads...\")\n",
        "        # Screen Capture Thread\n",
        "        self.capture_thread = ScreenCaptureThread(self.selected_monitor_spec)\n",
        "        self.capture_thread.status_update.connect(self.update_status)\n",
        "        self.capture_thread.start()\n",
        "        self.capture_thread.setObjectName(\"CaptureThread\") # For logging\n",
        "\n",
        "        # Detection Thread\n",
        "        self.detection_thread = DetectionThread(OBJECT_DETECTION_MODEL)\n",
        "        self.detection_thread.status_update.connect(self.update_status)\n",
        "        self.detection_thread.start()\n",
        "        self.detection_thread.setObjectName(\"DetectionThread\")\n",
        "\n",
        "        # Pose Estimation Thread\n",
        "        self.pose_thread = PoseEstimationThread(POSE_ESTIMATION_MODEL)\n",
        "        self.pose_thread.status_update.connect(self.update_status)\n",
        "        self.pose_thread.start()\n",
        "        self.pose_thread.setObjectName(\"PoseThread\")\n",
        "\n",
        "        # Optical Flow Thread\n",
        "        self.flow_thread = OpticalFlowThread()\n",
        "        self.flow_thread.status_update.connect(self.update_status)\n",
        "        self.flow_thread.start()\n",
        "        self.flow_thread.setObjectName(\"FlowThread\")\n",
        "\n",
        "        # Depth Estimation Thread\n",
        "        self.depth_thread = DepthEstimationThread() # Uses default MiDaS_small\n",
        "        self.depth_thread.status_update.connect(self.update_status)\n",
        "        self.depth_thread.start()\n",
        "        self.depth_thread.setObjectName(\"DepthThread\")\n",
        "\n",
        "        logger.info(\"Worker threads initialized and started.\")\n",
        "\n",
        "\n",
        "    def connect_signals(self):\n",
        "         \"\"\"Connects signals between GUI controls, worker threads, and the overlay.\"\"\"\n",
        "         logger.info(\"Connecting signals...\")\n",
        "         # Disconnect existing connections first to prevent duplicates if called multiple times\n",
        "         try:\n",
        "             # --- Capture Thread Connections ---\n",
        "             if self.capture_thread:\n",
        "                 self.capture_thread.frame_ready.disconnect() # Disconnect all slots from this signal\n",
        "                 self.capture_interval_changed.disconnect()\n",
        "             # --- Detection Thread Connections ---\n",
        "             if self.detection_thread:\n",
        "                 self.detection_thread.detections_ready.disconnect()\n",
        "                 self.detection_enabled_changed.disconnect()\n",
        "                 self.detection_conf_changed.disconnect()\n",
        "                 self.detection_nms_changed.disconnect()\n",
        "             # --- Pose Thread Connections ---\n",
        "             if self.pose_thread:\n",
        "                 self.pose_thread.poses_ready.disconnect()\n",
        "                 self.pose_enabled_changed.disconnect()\n",
        "                 self.pose_conf_changed.disconnect()\n",
        "             # --- Flow Thread Connections ---\n",
        "             if self.flow_thread:\n",
        "                 self.flow_thread.flow_ready.disconnect()\n",
        "                 self.flow_enabled_changed.disconnect()\n",
        "             # --- Depth Thread Connections ---\n",
        "             if self.depth_thread:\n",
        "                 self.depth_thread.depth_ready.disconnect()\n",
        "                 self.depth_enabled_changed.disconnect()\n",
        "             # --- Overlay Visibility Connections ---\n",
        "             self.toggle_boxes_signal.disconnect()\n",
        "             self.toggle_labels_signal.disconnect()\n",
        "             self.toggle_paths_signal.disconnect()\n",
        "             self.toggle_trajectory_signal.disconnect()\n",
        "             self.toggle_crosshair_signal.disconnect()\n",
        "             self.toggle_poses_signal.disconnect()\n",
        "             self.toggle_flow_signal.disconnect()\n",
        "             self.toggle_depth_signal.disconnect()\n",
        "             self.toggle_hud_signal.disconnect()\n",
        "             # --- HUD Data Connection ---\n",
        "             self.hud_data_signal.disconnect()\n",
        "             self.capture_fps_signal.disconnect()\n",
        "         except (TypeError, RuntimeError) as e:\n",
        "              # Ignore \"signal has no slots\" errors during the first connection setup\n",
        "              # logger.debug(f\"Error during signal disconnection (expected on first run): {e}\")\n",
        "              pass\n",
        "\n",
        "\n",
        "         # --- Capture Thread Connections ---\n",
        "         if self.capture_thread:\n",
        "             # Frame processing pipeline\n",
        "             self.capture_thread.frame_ready.connect(self.calculate_capture_fps) # Calculate FPS first\n",
        "             # Send frame to worker threads that need it\n",
        "             if self.detection_thread:\n",
        "                 self.capture_thread.frame_ready.connect(self.detection_thread.set_frame)\n",
        "             if self.pose_thread:\n",
        "                 self.capture_thread.frame_ready.connect(self.pose_thread.set_frame)\n",
        "             if self.flow_thread:\n",
        "                 self.capture_thread.frame_ready.connect(self.flow_thread.set_frame)\n",
        "             if self.depth_thread:\n",
        "                 self.capture_thread.frame_ready.connect(self.depth_thread.set_frame)\n",
        "             # Connect GUI controls to capture thread slots\n",
        "             self.capture_interval_changed.connect(self.capture_thread.update_capture_interval)\n",
        "\n",
        "\n",
        "         # --- Detection Thread Connections ---\n",
        "         if self.detection_thread:\n",
        "             self.detection_thread.detections_ready.connect(self.overlay.update_detections)\n",
        "             # Connect GUI controls to detection thread slots\n",
        "             self.detection_enabled_changed.connect(self.detection_thread.set_enabled)\n",
        "             self.detection_conf_changed.connect(self.detection_thread.update_confidence_threshold)\n",
        "             self.detection_nms_changed.connect(self.detection_thread.update_nms_threshold)\n",
        "             # Sync initial state from GUI to thread\n",
        "             self.detection_thread.set_enabled(self.detection_enabled_check.isChecked())\n",
        "             self.detection_thread.update_confidence_threshold(self.conf_spinbox.value())\n",
        "             self.detection_thread.update_nms_threshold(self.nms_spinbox.value())\n",
        "\n",
        "\n",
        "         # --- Pose Thread Connections ---\n",
        "         if self.pose_thread:\n",
        "             self.pose_thread.poses_ready.connect(self.overlay.update_poses)\n",
        "             # Connect GUI controls to pose thread slots\n",
        "             self.pose_enabled_changed.connect(self.pose_thread.set_enabled)\n",
        "             self.pose_conf_changed.connect(self.pose_thread.update_confidence_threshold)\n",
        "             # Sync initial state from GUI to thread\n",
        "             self.pose_thread.set_enabled(self.pose_enabled_check.isChecked())\n",
        "             self.pose_thread.update_confidence_threshold(self.pose_conf_spinbox.value())\n",
        "\n",
        "\n",
        "         # --- Flow Thread Connections ---\n",
        "         if self.flow_thread:\n",
        "             self.flow_thread.flow_ready.connect(self.overlay.update_flow)\n",
        "             # Connect GUI controls to flow thread slots\n",
        "             self.flow_enabled_changed.connect(self.flow_thread.set_enabled)\n",
        "             # Sync initial state from GUI to thread\n",
        "             self.flow_thread.set_enabled(self.flow_enabled_check.isChecked())\n",
        "\n",
        "\n",
        "         # --- Depth Thread Connections ---\n",
        "         if self.depth_thread:\n",
        "             self.depth_thread.depth_ready.connect(self.overlay.update_depth)\n",
        "             # Connect GUI controls to depth thread slots\n",
        "             self.depth_enabled_changed.connect(self.depth_thread.set_enabled)\n",
        "             # Sync initial state from GUI to thread\n",
        "             self.depth_thread.set_enabled(self.depth_enabled_check.isChecked())\n",
        "\n",
        "\n",
        "         # --- Overlay Visibility Connections (GUI Checkbox -> Overlay Slot) ---\n",
        "         self.toggle_boxes_signal.connect(self.overlay.toggle_boxes)\n",
        "         self.toggle_labels_signal.connect(self.overlay.toggle_labels)\n",
        "         self.toggle_paths_signal.connect(self.overlay.toggle_paths)\n",
        "         self.toggle_trajectory_signal.connect(self.overlay.toggle_trajectory)\n",
        "         self.toggle_crosshair_signal.connect(self.overlay.toggle_crosshair)\n",
        "         self.toggle_poses_signal.connect(self.overlay.toggle_poses) # Connect pose visibility\n",
        "         self.toggle_flow_signal.connect(self.overlay.toggle_flow)\n",
        "         self.toggle_depth_signal.connect(self.overlay.toggle_depth)\n",
        "         self.toggle_hud_signal.connect(self.overlay.toggle_hud)\n",
        "         # Sync initial state from GUI checkboxes to overlay drawing flags\n",
        "         self.overlay.toggle_boxes(self.show_boxes_check.isChecked())\n",
        "         self.overlay.toggle_labels(self.show_labels_check.isChecked())\n",
        "         self.overlay.toggle_paths(self.show_paths_check.isChecked())\n",
        "         self.overlay.toggle_trajectory(self.show_trajectory_check.isChecked())\n",
        "         self.overlay.toggle_crosshair(self.show_crosshair_check.isChecked())\n",
        "         self.overlay.toggle_poses(self.pose_enabled_check.isChecked()) # Link initial pose visibility to enable state\n",
        "         self.overlay.toggle_flow(self.flow_enabled_check.isChecked()) # Link initial flow visibility to enable state\n",
        "         self.overlay.toggle_depth(self.depth_enabled_check.isChecked()) # Link initial depth visibility to enable state\n",
        "         self.overlay.toggle_hud(self.show_hud_check.isChecked())\n",
        "\n",
        "\n",
        "         # --- HUD Data Connection (Main Window -> Overlay) ---\n",
        "         self.hud_data_signal.connect(self.overlay.update_hud)\n",
        "         self.capture_fps_signal.connect(self.overlay.update_capture_fps)\n",
        "\n",
        "\n",
        "         logger.info(\"Signals connected/reconnected.\")\n",
        "\n",
        "\n",
        "    def stop_threads(self):\n",
        "        \"\"\"Stops all worker threads gracefully.\"\"\"\n",
        "        logger.info(\"Stopping worker threads...\")\n",
        "        threads = [\n",
        "            self.capture_thread, self.detection_thread, self.pose_thread,\n",
        "            self.flow_thread, self.depth_thread\n",
        "        ]\n",
        "        for thread in threads:\n",
        "            if thread and thread.isRunning():\n",
        "                try:\n",
        "                    thread.stop() # Signal thread to stop\n",
        "                    if not thread.wait(2000): # Wait up to 2 seconds\n",
        "                         logger.warning(f\"Thread {thread.objectName()} did not finish gracefully, terminating.\")\n",
        "                         thread.terminate() # Force terminate if needed\n",
        "                         thread.wait() # Wait after termination\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error stopping thread {thread.objectName()}: {e}\")\n",
        "\n",
        "        # Clear thread references\n",
        "        self.capture_thread = None\n",
        "        self.detection_thread = None\n",
        "        self.pose_thread = None\n",
        "        self.flow_thread = None\n",
        "        self.depth_thread = None\n",
        "        logger.info(\"Worker threads stopped.\")\n",
        "\n",
        "\n",
        "    @Slot(str)\n",
        "    def update_status(self, message):\n",
        "        \"\"\"Updates the status bar message.\"\"\"\n",
        "        self.status_bar.showMessage(message, 5000) # Show for 5 seconds\n",
        "\n",
        "\n",
        "    def update_system_info(self):\n",
        "        \"\"\"Fetches system resource usage and sends it to the HUD via signal.\"\"\"\n",
        "        try:\n",
        "            cpu_usage = psutil.cpu_percent()\n",
        "            memory_info = psutil.virtual_memory()\n",
        "            mem_usage = f\"{memory_info.percent}% ({memory_info.used / (1024**3):.1f}/{memory_info.total / (1024**3):.1f} GB)\"\n",
        "            gpu_name, gpu_mem_usage = get_gpu_info() # Fetch GPU info\n",
        "\n",
        "            hud_data = {\n",
        "                \"CPU\": f\"{cpu_usage:.1f}%\",\n",
        "                \"RAM\": mem_usage,\n",
        "                \"GPU\": gpu_name,\n",
        "                \"VRAM\": gpu_mem_usage,\n",
        "                \"Time\": datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
        "            }\n",
        "            self.hud_data_signal.emit(hud_data) # Emit data for overlay\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error updating system info: {e}\", exc_info=False) # Log error briefly\n",
        "\n",
        "\n",
        "    # --- FPS Calculation Slot ---\n",
        "    @Slot(np.ndarray, float)\n",
        "    def calculate_capture_fps(self, frame, timestamp):\n",
        "        \"\"\"Calculates capture FPS based on frame arrival times and emits it.\"\"\"\n",
        "        now = time.perf_counter()\n",
        "        # Use overlay's time tracker for consistency\n",
        "        time_diff = now - self.overlay.last_capture_time\n",
        "        if time_diff > 1e-6: # Avoid division by zero or near-zero\n",
        "            fps = 1.0 / time_diff\n",
        "            self.capture_fps_signal.emit(fps) # Emit the calculated FPS\n",
        "        self.overlay.last_capture_time = now # Update the last time stamp\n",
        "\n",
        "\n",
        "    def closeEvent(self, event):\n",
        "        \"\"\"Handles the main window closing event to ensure clean shutdown.\"\"\"\n",
        "        logger.info(\"Close event triggered. Cleaning up...\")\n",
        "        self.stop_threads() # Stop worker threads first\n",
        "        if self.overlay:\n",
        "            self.overlay.close() # Close the overlay window\n",
        "        logger.info(\"Cleanup complete. Exiting.\")\n",
        "        event.accept() # Accept the close event\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Pre-check/Download Models ---\n",
        "    # Ensure necessary model files are present or downloaded by ultralytics\n",
        "    # This avoids potential delays or errors during thread initialization.\n",
        "    models_to_check = [OBJECT_DETECTION_MODEL, POSE_ESTIMATION_MODEL]\n",
        "    logger.info(\"Checking for required YOLO models...\")\n",
        "    for model_file in models_to_check:\n",
        "        try:\n",
        "            if not os.path.exists(model_file):\n",
        "                 logger.info(f\"Model '{model_file}' not found. Attempting to download...\")\n",
        "                 _ = YOLO(model_file) # Instantiating YOLO class should trigger download\n",
        "                 logger.info(f\"Model '{model_file}' download attempt complete.\")\n",
        "            else:\n",
        "                 logger.info(f\"Model '{model_file}' found.\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not pre-download/verify model '{model_file}': {e}. YOLO will attempt download on first use.\", exc_info=False)\n",
        "\n",
        "    # --- Application Setup ---\n",
        "    # Enable High DPI scaling for better rendering on high-resolution displays\n",
        "    QApplication.setAttribute(Qt.AA_EnableHighDpiScaling, True)\n",
        "    QApplication.setAttribute(Qt.AA_UseHighDpiPixmaps, True)\n",
        "\n",
        "    app = QApplication(sys.argv)\n",
        "\n",
        "    # --- Font Loading ---\n",
        "    # Ensure the custom font is loaded before creating widgets that use it\n",
        "    font_path = \"PressStart2P-Regular.ttf\"\n",
        "    if os.path.exists(font_path):\n",
        "        font_id = QFontDatabase.addApplicationFont(font_path)\n",
        "        if font_id == -1:\n",
        "             logger.warning(f\"Failed to load application font: {font_path}\")\n",
        "    else:\n",
        "        logger.warning(f\"Font file not found: {font_path}. Will use fallback.\")\n",
        "\n",
        "\n",
        "    # --- Main Window Initialization ---\n",
        "    main_window = MainWindow() # Create the main window instance\n",
        "    main_window.show() # Show the main window\n",
        "\n",
        "    # --- Start Event Loop ---\n",
        "    sys.exit(app.exec()) # Execute the application event loop"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "RorbK2GdY8Ew"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}